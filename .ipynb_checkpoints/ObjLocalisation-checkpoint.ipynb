{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.23) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./lib/\")\n",
    "\n",
    "import VOC2012DataProvider\n",
    "import VOC2012_npz_files_writter\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Agent import ObjLocaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records are already prepared!!!\n"
     ]
    }
   ],
   "source": [
    "#This cell reads VOC 2012 dataset and save them in .npz files for future.\n",
    "#The process of reading data and put them in prper format is time consuming so they are stored in a file.\n",
    "\n",
    "xml_path = \"../VOC2012/Annotations/*.xml\"\n",
    "destination = \"../data/\"\n",
    "\n",
    "#It splits dataset to 80% for training and 20% validation.\n",
    "if not (os.path.isfile(destination+\"test_input.npz\") or os.path.isfile(destination+\"test_target.npz\")):\n",
    "    VOC2012_npz_files_writter.writting_files(xml_path, destination, percentage=0)\n",
    "    print(\"Files are ready!!!\")\n",
    "else:\n",
    "    print(\"Records are already prepared!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = VOC2012DataProvider.PascalDataProvider('train', batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.# Convo \n",
    "filter_size1 = 7          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 96         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 7          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 265        # There are 36 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size2 = 3          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 384        # There are 36 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 384        # There are 36 of these filters.\n",
    "\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc1_size = 4096           # Number of neurons in fully-connected layer.\n",
    "fc2_size = 1024           # Number of neurons in fully-connected layer.\n",
    "fc3_size = 1024           # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 224\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_actions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05),name='W')\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   filter_stride,\n",
    "                   use_pooling,   # Use max-pooling.\n",
    "                   pooling_size = None,\n",
    "                   pooling_stride = None,\n",
    "                   name = 'Conv'):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        # Shape of the filter-weights for the convolution.\n",
    "        # This format is determined by the TensorFlow API.\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "        # Create new weights aka. filters with the given shape.\n",
    "        weights = new_weights(shape=shape)\n",
    "\n",
    "        # Create new biases, one for each filter.\n",
    "        biases = new_biases(length=num_filters)\n",
    "\n",
    "        # Create the TensorFlow operation for convolution.\n",
    "        # Note the strides are set to 1 in all dimensions.\n",
    "        # The first and last stride must always be 1,\n",
    "        # because the first is for the image-number and\n",
    "        # the last is for the input-channel.\n",
    "        # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "        # is moved 2 pixels across the x- and y-axis of the image.\n",
    "        # The padding is set to 'SAME' which means the input image\n",
    "        # is padded with zeroes so the size of the output is the same.\n",
    "        layer = tf.nn.conv2d(input=input,\n",
    "                             filter=weights,\n",
    "                             strides=[1, filter_stride, filter_stride, 1], #strides=[batch, height, width, channels]\n",
    "                             padding='SAME') # 'SAME' means zero-padding\n",
    "\n",
    "        # Add the biases to the results of the convolution.\n",
    "        # A bias-value is added to each filter-channel.\n",
    "        layer += biases\n",
    "\n",
    "        # Use pooling to down-sample the image resolution?\n",
    "        if use_pooling:\n",
    "            # This is 2x2 max-pooling, which means that we\n",
    "            # consider 2x2 windows and select the largest value\n",
    "            # in each window. Then we move 2 pixels to the next window.\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, pooling_size, pooling_size, 1],\n",
    "                                   strides=[1, pooling_stride, pooling_stride, 1], #strides=[batch, height, width, channels]\n",
    "                                   padding='SAME') # 'SAME' means zero-padding\n",
    "\n",
    "        # Rectified Linear Unit (ReLU).\n",
    "        # It calculates max(x, 0) for each input pixel x.\n",
    "        # This adds some non-linearity to the formula and allows us\n",
    "        # to learn more complicated functions.\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        tf.summary.histogram(\"activations\", layer)\n",
    "\n",
    "        # Note that ReLU is normally executed before the pooling,\n",
    "        # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "        # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "        # We return both the resulting layer and the filter-weights\n",
    "        # because we will plot the weights later.\n",
    "        return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,\n",
    "                 name = 'fc'): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        # Create new weights and biases.\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "        biases = new_biases(length=num_outputs)\n",
    "\n",
    "        # Calculate the layer as the matrix multiplication of\n",
    "        # the input and weights, and then add the bias-values.\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        # Use ReLU?\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "            \n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        tf.summary.histogram(\"activations\", layer)\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[1, img_size, img_size, num_channels], name='x')\n",
    "#x = tf.placeholder(tf.float32, shape=[img_size, img_size, num_channels], name='x')\n",
    "\n",
    "#x_image = tf.reshape(x, [1, img_size, img_size, num_channels])\n",
    "#tf.summary.image('input', x_image, 1)\n",
    "tf.summary.image('input', x, 1)\n",
    "\n",
    "\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, num_actions], name='y_true')\n",
    "#y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x,\n",
    "                   num_input_channels=3,\n",
    "                   filter_size=7,\n",
    "                   num_filters=96,\n",
    "                   filter_stride = 2,\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 2,\n",
    "                   name = 'conv1')\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=96,\n",
    "                   filter_size=5,\n",
    "                   num_filters=256,\n",
    "                   filter_stride = 2,\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 1,\n",
    "                   name = 'conv2')\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "    new_conv_layer(input=layer_conv2,\n",
    "                   num_input_channels=256,\n",
    "                   filter_size=3,\n",
    "                   num_filters=384,\n",
    "                   filter_stride = 1,\n",
    "                   use_pooling=False,   # Use max-pooling.\n",
    "                   name = 'conv3')\n",
    "\n",
    "layer_conv4, weights_conv4 = \\\n",
    "    new_conv_layer(input=layer_conv3,\n",
    "                   num_input_channels=384,\n",
    "                   filter_size=3,\n",
    "                   num_filters=384,\n",
    "                   filter_stride = 1,\n",
    "                   use_pooling=False,  \n",
    "                   name = 'conv4')\n",
    "\n",
    "layer_conv5, weights_conv5 = \\\n",
    "    new_conv_layer(input=layer_conv4,\n",
    "                   num_input_channels=384,\n",
    "                   filter_size=0,\n",
    "                   num_filters=0,\n",
    "                   filter_stride = 1, #it should be 0 but becuase number of filter is 0 there shouldn't be any problem\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 2,\n",
    "                   name = 'conv5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv5)\n",
    "\n",
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=1024,\n",
    "                         use_relu=True,\n",
    "                         name = 'fc1')\n",
    "\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=1024,\n",
    "                         num_outputs=1024,\n",
    "                         use_relu=False,\n",
    "                         name = 'fc2')\n",
    "\n",
    "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                         num_inputs=1024,\n",
    "                         num_outputs=num_actions,\n",
    "                         use_relu=False,\n",
    "                         name = 'fc3' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qout = tf.nn.softmax(layer_fc3)\n",
    "#y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=y_true)\n",
    "#cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,num_actions],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizeroptimize  = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "#correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\n\t [[Node: Reshape_8 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv5_4/Relu, Reshape_8/shape)]]\n\nCaused by op u'Reshape_8', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/mohammad/Tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-57-6c7c898a41cd>\", line 1, in <module>\n    layer_flat, num_features = flatten_layer(layer_conv5)\n  File \"<ipython-input-21-2a7c01df0ce9>\", line 17, in flatten_layer\n    layer_flat = tf.reshape(layer, [-1, num_features])\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\n\t [[Node: Reshape_8 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv5_4/Relu, Reshape_8/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a26f86b694ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mallQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\n\t [[Node: Reshape_8 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv5_4/Relu, Reshape_8/shape)]]\n\nCaused by op u'Reshape_8', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/mohammad/Tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 589, in launch_instance\n    app.start()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-57-6c7c898a41cd>\", line 1, in <module>\n    layer_flat, num_features = flatten_layer(layer_conv5)\n  File \"<ipython-input-21-2a7c01df0ce9>\", line 17, in flatten_layer\n    layer_flat = tf.reshape(layer, [-1, num_features])\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6113, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\n\t [[Node: Reshape_8 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv5_4/Relu, Reshape_8/shape)]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for input_batch, target_batch in inputs:\n",
    "    for i in range(len(input_batch)):\n",
    "        #first_obj = [target_batch[i]['xmax'][0], target_batch[i]['xmin'][0], target_batch[i]['ymax'][0], target_batch[i]['ymin'][0]]\n",
    "        PILimage = Image.frombytes(\"RGB\",(inputs.inputs[i]['image_width'],inputs.inputs[i]['image_height']),inputs.inputs[i]['image'])\n",
    "        #ObjLocaliser object should be initialised with a PIL image object and a dictionary of bounding boxes\n",
    "        dqn_agent = ObjLocaliser(PILimage, target_batch[i])\n",
    "\n",
    "        for _ in range(50):\n",
    "            \n",
    "            s = np.array(dqn_agent.wrapping()).reshape((1,224,224,3))\n",
    "            print s.shape\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={x: s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(10, size=1)\n",
    "            #Get new state and reward from environment\n",
    "            r = dqn_agent.takingActions(a[0])\n",
    "            s1 = np.array(dqn_agent.wrapping()).reshape((1,224,224,3))\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:s1})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _ = sess.run(updateModel,feed_dict={inputs1:s, nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            #if d == True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                #e = 1./((i/50) + 10)\n",
    "                #break\n",
    "            #jList.append(j)\n",
    "            #rList.append(rAll)\n",
    "            \n",
    "            \n",
    "            #_, batch_error, batch_acc = sess.run(\n",
    "            #    [optimizer, cost, accuracy], \n",
    "            #    feed_dict={x: input_batch[i]['image']})\n",
    "        dqn_agent.drawActions()\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= np.array(dqn_agent.wrapping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=s.reshape((1,224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(10):\n",
    "    \n",
    "    running_error = 0.\n",
    "    running_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [optimizer, cost, accuracy], \n",
    "            feed_dict={x: input_batch, y_true: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_accuracy += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_accuracy /= train_data.num_batches\n",
    "    print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f}'\n",
    "          .format(e + 1, running_error, running_accuracy))\n",
    "    if (e + 1) % 5 == 0:\n",
    "        valid_error = 0.\n",
    "        valid_accuracy = 0.\n",
    "        for input_batch, target_batch in valid_data:\n",
    "            batch_error, batch_acc = sess.run([cost, accuracy],feed_dict={x: input_batch, y_true: target_batch})\n",
    "            valid_error += batch_error\n",
    "            valid_accuracy += batch_acc\n",
    "        valid_error /= valid_data.num_batches\n",
    "        valid_accuracy /= valid_data.num_batches\n",
    "        print('                 err(valid)={0:.2f} acc(valid)={1:.2f}'\n",
    "               .format(valid_error, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to load data from pascal VOC: https://gist.github.com/saghiralfasly/ee642af0616461145a9a82d7317fb1d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing IoU: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "https://angusg.com/writing/2016/12/28/optimizing-iou-semantic-segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
