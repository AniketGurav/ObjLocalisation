{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "if \"./lib\" not in sys.path:\n",
    "    sys.path.append(\"./lib\")\n",
    "    \n",
    "import plotting\n",
    "from collections import deque, namedtuple\n",
    "from readingFileEfficiently import *\n",
    "import VOC2012_npz_files_writter\n",
    "from DNN import *\n",
    "from Agent import ObjLocaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records are already prepared!!!\n"
     ]
    }
   ],
   "source": [
    "#This cell reads VOC 2012 dataset and save them in .npz files for future.\n",
    "#The process of reading data and put them in prper format is time consuming so they are stored in a file.\n",
    "\n",
    "xml_path = \"../VOC2012/Annotations/*.xml\"\n",
    "destination = \"../data/\"\n",
    "\n",
    "#It splits dataset to 80% for training and 20% validation.\n",
    "if not (os.path.isfile(destination+\"test_input.npz\") or os.path.isfile(destination+\"test_target.npz\")):\n",
    "    VOC2012_npz_files_writter.writting_files(xml_path, destination, percentage=0)\n",
    "    print(\"Files are ready!!!\")\n",
    "else:\n",
    "    print(\"Records are already prepared!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./lib/DNN.py:117: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file 1 is loading...\n",
      "New image is being loaded: 2010_002425.jpg\n",
      "Populating replay memory...\n",
      "\n",
      "\n",
      "Copied model parameters to target network.\n",
      "[0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n",
      " 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]\n",
      "Step 0 (0) @ Episode 1/10, action 10, reward 3,loss: 1.49924337864\n",
      "Episode Reward: 3 Episode Length: 1\n",
      "[0.09090893 0.09090893 0.09090893 0.09090893 0.09090893 0.09090893\n",
      " 0.09090893 0.09090893 0.09090893 0.09091073 0.09090893]\n",
      "Step 0 (1) @ Episode 2/10, action 6, reward 1,loss: 1.52652263641\n",
      "[0.09090876 0.09090876 0.09090876 0.09090876 0.09090876 0.09090876\n",
      " 0.09090876 0.09090876 0.09090876 0.09091236 0.09090876]\n",
      "Step 1 (2) @ Episode 2/10, action 2, reward -1,loss: 1.52098870277\n",
      "[0.0909086 0.0909086 0.0909086 0.0909086 0.0909086 0.0909086 0.0909086\n",
      " 0.0909086 0.0909086 0.090914  0.0909086]\n",
      "Step 2 (3) @ Episode 2/10, action 6, reward 1,loss: 3.0085272789\n",
      "[0.09090844 0.09090844 0.09090844 0.09090844 0.09090844 0.09090844\n",
      " 0.09090844 0.09090844 0.09090844 0.09091564 0.09090844]\n",
      "Step 3 (4) @ Episode 2/10, action 5, reward -1,loss: 1.75051248074\n",
      "[0.09090827 0.09090827 0.09090827 0.09090827 0.09090827 0.09090827\n",
      " 0.09090827 0.09090827 0.09090827 0.09091727 0.09090827]\n",
      "Step 4 (5) @ Episode 2/10, action 9, reward -1,loss: 1.99903297424\n",
      "[0.09090811 0.09090811 0.09090811 0.09090811 0.09090811 0.09090811\n",
      " 0.09090811 0.09090811 0.09090811 0.09091891 0.09090811]\n",
      "Step 5 (6) @ Episode 2/10, action 4, reward 1,loss: 1.46273052692\n",
      "[0.09090795 0.09090795 0.09090795 0.09090795 0.09090795 0.09090795\n",
      " 0.09090795 0.09090795 0.09090795 0.09092055 0.09090795]\n",
      "Step 6 (7) @ Episode 2/10, action 2, reward 1,loss: 1.45454621315\n",
      "[0.09090778 0.09090778 0.09090778 0.09090778 0.09090778 0.09090778\n",
      " 0.09090778 0.09090778 0.09090778 0.09092218 0.09090778]\n",
      "Step 7 (8) @ Episode 2/10, action 1, reward 1,loss: 1.94964432716\n",
      "[0.09090762 0.09090762 0.09090762 0.09090762 0.09090762 0.09090762\n",
      " 0.09090762 0.09090762 0.09090762 0.09092382 0.09090762]\n",
      "Step 8 (9) @ Episode 2/10, action 6, reward -1,loss: 2.01624464989\n",
      "[0.09090745 0.09090745 0.09090745 0.09090745 0.09090745 0.09090745\n",
      " 0.09090745 0.09090745 0.09090745 0.09092545 0.09090745]\n",
      "Step 9 (10) @ Episode 2/10, action 7, reward 1,loss: 1.78056752682\n",
      "[0.09090729 0.09090729 0.09090729 0.09092709 0.09090729 0.09090729\n",
      " 0.09090729 0.09090729 0.09090729 0.09090729 0.09090729]\n",
      "Step 10 (11) @ Episode 2/10, action 9, reward -1,loss: 1.5322343111\n",
      "[0.09090713 0.09090713 0.09090713 0.09090713 0.09090713 0.09090713\n",
      " 0.09090713 0.09090713 0.09090713 0.09092873 0.09090713]\n",
      "Step 11 (12) @ Episode 2/10, action 8, reward -1,loss: 1.48053741455\n",
      "[0.09090696 0.09090696 0.09090696 0.09090696 0.09090696 0.09090696\n",
      " 0.09090696 0.09090696 0.09090696 0.09093036 0.09090696]\n",
      "Step 12 (13) @ Episode 2/10, action 9, reward -1,loss: 1.52876496315\n",
      "[0.0909068 0.0909068 0.0909068 0.0909068 0.0909068 0.090932  0.0909068\n",
      " 0.0909068 0.0909068 0.0909068 0.0909068]\n",
      "Step 13 (14) @ Episode 2/10, action 6, reward -1,loss: 1.74352347851\n",
      "[0.09090664 0.09090664 0.09090664 0.09093364 0.09090664 0.09090664\n",
      " 0.09090664 0.09090664 0.09090664 0.09090664 0.09090664]\n",
      "Step 14 (15) @ Episode 2/10, action 6, reward -1,loss: 2.2998380661\n",
      "[0.09090647 0.09090647 0.09090647 0.09090647 0.09090647 0.09093527\n",
      " 0.09090647 0.09090647 0.09090647 0.09090647 0.09090647]\n",
      "Step 15 (16) @ Episode 2/10, action 8, reward -1,loss: 2.2553126812\n",
      "[0.09090631 0.09090631 0.09090631 0.09090631 0.09090631 0.09090631\n",
      " 0.09090631 0.09090631 0.09090631 0.09093691 0.09090631]\n",
      "Step 16 (17) @ Episode 2/10, action 4, reward -1,loss: 2.21297287941\n",
      "[0.09090615 0.09090615 0.09090615 0.09093855 0.09090615 0.09090615\n",
      " 0.09090615 0.09090615 0.09090615 0.09090615 0.09090615]\n",
      "Step 17 (18) @ Episode 2/10, action 6, reward -1,loss: 1.6778241396\n",
      "[0.09090598 0.09090598 0.09090598 0.09090598 0.09090598 0.09094018\n",
      " 0.09090598 0.09090598 0.09090598 0.09090598 0.09090598]\n",
      "Step 18 (19) @ Episode 2/10, action 0, reward 1,loss: 1.74320149422\n",
      "[0.09090582 0.09090582 0.09090582 0.09094182 0.09090582 0.09090582\n",
      " 0.09090582 0.09090582 0.09090582 0.09090582 0.09090582]\n",
      "Step 19 (20) @ Episode 2/10, action 10, reward -3,loss: 1.7404358387\n",
      "Episode Reward: -8 Episode Length: 20\n",
      "[0.09090565 0.09090565 0.09090565 0.09090565 0.09090565 0.09090565\n",
      " 0.09090565 0.09090565 0.09090565 0.09094345 0.09090565]\n",
      "Step 0 (21) @ Episode 3/10, action 0, reward 1,loss: 1.47060275078\n",
      "[0.09090549 0.09090549 0.09090549 0.09090549 0.09090549 0.09090549\n",
      " 0.09090549 0.09090549 0.09090549 0.09094509 0.09090549]\n",
      "Step 1 (22) @ Episode 3/10, action 2, reward -1,loss: 2.00768065453\n",
      "[0.09090533 0.09090533 0.09090533 0.09090533 0.09090533 0.09090533\n",
      " 0.09090533 0.09090533 0.09090533 0.09094673 0.09090533]\n",
      "Step 2 (23) @ Episode 3/10, action 10, reward 3,loss: 1.24809968472\n",
      "Episode Reward: 3 Episode Length: 3\n",
      "[0.09090516 0.09090516 0.09090516 0.09090516 0.09090516 0.09090516\n",
      " 0.09090516 0.09090516 0.09090516 0.09094836 0.09090516]\n",
      "Step 0 (24) @ Episode 4/10, action 8, reward 1,loss: 1.75900352001\n",
      "[0.090905 0.090905 0.090905 0.090905 0.090905 0.090905 0.090905 0.090905\n",
      " 0.090905 0.09095  0.090905]\n",
      "Step 1 (25) @ Episode 4/10, action 4, reward -1,loss: 1.7306791544\n",
      "[0.09090484 0.09090484 0.09090484 0.09090484 0.09090484 0.09090484\n",
      " 0.09090484 0.09090484 0.09090484 0.09095164 0.09090484]\n",
      "Step 2 (26) @ Episode 4/10, action 1, reward 1,loss: 0.960504829884\n",
      "[0.09090467 0.09090467 0.09090467 0.09090467 0.09090467 0.09090467\n",
      " 0.09090467 0.09090467 0.09090467 0.09095327 0.09090467]\n",
      "Step 3 (27) @ Episode 4/10, action 2, reward -1,loss: 2.59144186974\n",
      "[0.09090451 0.09090451 0.09090451 0.09090451 0.09090451 0.09090451\n",
      " 0.09090451 0.09090451 0.09090451 0.09095491 0.09090451]\n",
      "Step 4 (28) @ Episode 4/10, action 5, reward -1,loss: 1.98663008213\n",
      "[0.09090435 0.09090435 0.09090435 0.09090435 0.09090435 0.09090435\n",
      " 0.09090435 0.09090435 0.09090435 0.09095655 0.09090435]\n",
      "Step 5 (29) @ Episode 4/10, action 1, reward -1,loss: 1.50246047974\n",
      "[0.09090418 0.09090418 0.09090418 0.09090418 0.09090418 0.09090418\n",
      " 0.09090418 0.09090418 0.09090418 0.09095818 0.09090418]\n",
      "Step 6 (30) @ Episode 4/10, action 9, reward -1,loss: 2.02708768845\n",
      "[0.09090402 0.09090402 0.09090402 0.09090402 0.09090402 0.09095982\n",
      " 0.09090402 0.09090402 0.09090402 0.09090402 0.09090402]\n",
      "Step 7 (31) @ Episode 4/10, action 10, reward -3,loss: 1.76507806778\n",
      "Episode Reward: -6 Episode Length: 8\n",
      "[0.09090385 0.09090385 0.09090385 0.09096145 0.09090385 0.09090385\n",
      " 0.09090385 0.09090385 0.09090385 0.09090385 0.09090385]\n",
      "Step 0 (32) @ Episode 5/10, action 6, reward 1,loss: 1.76102018356\n",
      "[0.09090369 0.09090369 0.09090369 0.09090369 0.09090369 0.09090369\n",
      " 0.09090369 0.09090369 0.09090369 0.09096309 0.09090369]\n",
      "Step 1 (33) @ Episode 5/10, action 9, reward -1,loss: 1.73047840595\n",
      "[0.09090353 0.09090353 0.09090353 0.09096473 0.09090353 0.09090353\n",
      " 0.09090353 0.09090353 0.09090353 0.09090353 0.09090353]\n",
      "Step 2 (34) @ Episode 5/10, action 8, reward -1,loss: 1.25855588913\n",
      "[0.09090336 0.09090336 0.09090336 0.09096636 0.09090336 0.09090336\n",
      " 0.09090336 0.09090336 0.09090336 0.09090336 0.09090336]\n",
      "Step 3 (35) @ Episode 5/10, action 8, reward -1,loss: 1.75099539757\n",
      "[0.0909032 0.0909032 0.0909032 0.090968  0.0909032 0.0909032 0.0909032\n",
      " 0.0909032 0.0909032 0.0909032 0.0909032]\n",
      "Step 4 (36) @ Episode 5/10, action 10, reward -3,loss: 2.47132587433\n",
      "Episode Reward: -5 Episode Length: 5\n",
      "[0.09090304 0.09090304 0.09090304 0.09096964 0.09090304 0.09090304\n",
      " 0.09090304 0.09090304 0.09090304 0.09090304 0.09090304]\n",
      "Step 0 (37) @ Episode 6/10, action 0, reward 1,loss: 1.68206989765\n",
      "[0.09090287 0.09090287 0.09090287 0.09097127 0.09090287 0.09090287\n",
      " 0.09090287 0.09090287 0.09090287 0.09090287 0.09090287]\n",
      "Step 1 (38) @ Episode 6/10, action 7, reward -1,loss: 1.24125373363\n",
      "[0.09090271 0.09090271 0.09090271 0.09097291 0.09090271 0.09090271\n",
      " 0.09090271 0.09090271 0.09090271 0.09090271 0.09090271]\n",
      "Step 2 (39) @ Episode 6/10, action 0, reward -1,loss: 2.25155282021\n",
      "[0.09090255 0.09090255 0.09090255 0.09097455 0.09090255 0.09090255\n",
      " 0.09090255 0.09090255 0.09090255 0.09090255 0.09090255]\n",
      "Step 3 (40) @ Episode 6/10, action 0, reward -1,loss: 1.46117269993\n",
      "[0.09090238 0.09090238 0.09090238 0.09097618 0.09090238 0.09090238\n",
      " 0.09090238 0.09090238 0.09090238 0.09090238 0.09090238]\n",
      "Step 4 (41) @ Episode 6/10, action 1, reward 1,loss: 2.74409532547\n",
      "[0.09090222 0.09090222 0.09090222 0.09097782 0.09090222 0.09090222\n",
      " 0.09090222 0.09090222 0.09090222 0.09090222 0.09090222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 (42) @ Episode 6/10, action 9, reward -1,loss: 1.52020764351\n",
      "[0.09090205 0.09090205 0.09090205 0.09090205 0.09090205 0.09097945\n",
      " 0.09090205 0.09090205 0.09090205 0.09090205 0.09090205]\n",
      "Step 6 (43) @ Episode 6/10, action 5, reward -1,loss: 1.95092189312\n",
      "[0.09090189 0.09090189 0.09098109 0.09090189 0.09090189 0.09090189\n",
      " 0.09090189 0.09090189 0.09090189 0.09090189 0.09090189]\n",
      "Step 7 (44) @ Episode 6/10, action 9, reward -1,loss: 0.924188256264\n",
      "[0.09090173 0.09090173 0.09090173 0.09090173 0.09090173 0.09090173\n",
      " 0.09090173 0.09090173 0.09090173 0.09098273 0.09090173]\n",
      "Step 8 (45) @ Episode 6/10, action 10, reward -3,loss: 1.53539085388\n",
      "Episode Reward: -7 Episode Length: 9\n",
      "[0.09090156 0.09090156 0.09090156 0.09098436 0.09090156 0.09090156\n",
      " 0.09090156 0.09090156 0.09090156 0.09090156 0.09090156]\n",
      "Step 0 (46) @ Episode 7/10, action 1, reward 1,loss: 1.45343732834\n",
      "[0.0909014 0.0909014 0.0909014 0.090986  0.0909014 0.0909014 0.0909014\n",
      " 0.0909014 0.0909014 0.0909014 0.0909014]\n",
      "Step 1 (47) @ Episode 7/10, action 1, reward -1,loss: 2.72817277908\n",
      "[0.09090124 0.09090124 0.09090124 0.09098764 0.09090124 0.09090124\n",
      " 0.09090124 0.09090124 0.09090124 0.09090124 0.09090124]\n",
      "Step 2 (48) @ Episode 7/10, action 7, reward -1,loss: 2.186835289\n",
      "[0.09090107 0.09090107 0.09090107 0.09098927 0.09090107 0.09090107\n",
      " 0.09090107 0.09090107 0.09090107 0.09090107 0.09090107]\n",
      "Step 3 (49) @ Episode 7/10, action 4, reward -1,loss: 1.94383859634\n",
      "[0.09090091 0.09090091 0.09090091 0.09099091 0.09090091 0.09090091\n",
      " 0.09090091 0.09090091 0.09090091 0.09090091 0.09090091]\n",
      "Step 4 (50) @ Episode 7/10, action 2, reward -1,loss: 1.68848991394\n",
      "[0.09090075 0.09090075 0.09090075 0.09099255 0.09090075 0.09090075\n",
      " 0.09090075 0.09090075 0.09090075 0.09090075 0.09090075]\n",
      "Step 5 (51) @ Episode 7/10, action 7, reward -1,loss: 1.22226130962\n",
      "[0.09090058 0.09090058 0.09090058 0.09099418 0.09090058 0.09090058\n",
      " 0.09090058 0.09090058 0.09090058 0.09090058 0.09090058]\n",
      "Step 6 (52) @ Episode 7/10, action 8, reward -1,loss: 1.76064729691\n",
      "[0.09090042 0.09090042 0.09090042 0.09099582 0.09090042 0.09090042\n",
      " 0.09090042 0.09090042 0.09090042 0.09090042 0.09090042]\n",
      "Step 7 (53) @ Episode 7/10, action 10, reward 3,loss: 1.45536160469\n",
      "Episode Reward: -2 Episode Length: 8\n",
      "[0.09090025 0.09090025 0.09090025 0.09099745 0.09090025 0.09090025\n",
      " 0.09090025 0.09090025 0.09090025 0.09090025 0.09090025]\n",
      "Step 0 (54) @ Episode 8/10, action 4, reward 1,loss: 1.44700670242\n",
      "[0.09090009 0.09090009 0.09090009 0.09099909 0.09090009 0.09090009\n",
      " 0.09090009 0.09090009 0.09090009 0.09090009 0.09090009]\n",
      "Step 1 (55) @ Episode 8/10, action 2, reward -1,loss: 1.683552742\n",
      "[0.09089993 0.09089993 0.09089993 0.09100073 0.09089993 0.09089993\n",
      " 0.09089993 0.09089993 0.09089993 0.09089993 0.09089993]\n",
      "Step 2 (56) @ Episode 8/10, action 1, reward 1,loss: 3.01988601685\n",
      "[0.09089976 0.09089976 0.09089976 0.09100236 0.09089976 0.09089976\n",
      " 0.09089976 0.09089976 0.09089976 0.09089976 0.09089976]\n",
      "Step 3 (57) @ Episode 8/10, action 5, reward -1,loss: 2.50359392166\n",
      "[0.0908996 0.0908996 0.0908996 0.091004  0.0908996 0.0908996 0.0908996\n",
      " 0.0908996 0.0908996 0.0908996 0.0908996]\n",
      "Step 4 (58) @ Episode 8/10, action 1, reward -1,loss: 1.99566411972\n",
      "[0.09089944 0.09089944 0.09089944 0.09100564 0.09089944 0.09089944\n",
      " 0.09089944 0.09089944 0.09089944 0.09089944 0.09089944]\n",
      "Step 5 (59) @ Episode 8/10, action 2, reward -1,loss: 1.44406151772\n",
      "[0.09089927 0.09089927 0.09089927 0.09100727 0.09089927 0.09089927\n",
      " 0.09089927 0.09089927 0.09089927 0.09089927 0.09089927]\n",
      "Step 6 (60) @ Episode 8/10, action 9, reward -1,loss: 1.23439455032\n",
      "[0.09089911 0.09089911 0.09089911 0.09100891 0.09089911 0.09089911\n",
      " 0.09089911 0.09089911 0.09089911 0.09089911 0.09089911]\n",
      "Step 7 (61) @ Episode 8/10, action 0, reward 1,loss: 1.53068292141\n",
      "[0.09089895 0.09089895 0.09089895 0.09101055 0.09089895 0.09089895\n",
      " 0.09089895 0.09089895 0.09089895 0.09089895 0.09089895]\n",
      "Step 8 (62) @ Episode 8/10, action 1, reward 1,loss: 2.26900625229\n",
      "[0.09089878 0.09089878 0.09089878 0.09101218 0.09089878 0.09089878\n",
      " 0.09089878 0.09089878 0.09089878 0.09089878 0.09089878]\n",
      "Step 9 (63) @ Episode 8/10, action 4, reward -1,loss: 1.44994366169\n",
      "[0.09089862 0.09089862 0.09089862 0.09101382 0.09089862 0.09089862\n",
      " 0.09089862 0.09089862 0.09089862 0.09089862 0.09089862]\n",
      "Step 10 (64) @ Episode 8/10, action 6, reward -1,loss: 1.46571719646\n",
      "[0.09089845 0.09089845 0.09089845 0.09101545 0.09089845 0.09089845\n",
      " 0.09089845 0.09089845 0.09089845 0.09089845 0.09089845]\n",
      "Step 11 (65) @ Episode 8/10, action 3, reward 1,loss: 1.76458716393\n",
      "[0.09089829 0.09089829 0.09089829 0.09101709 0.09089829 0.09089829\n",
      " 0.09089829 0.09089829 0.09089829 0.09089829 0.09089829]\n",
      "Step 12 (66) @ Episode 8/10, action 9, reward -1,loss: 1.67249667645\n",
      "[0.09089813 0.09089813 0.09089813 0.09101873 0.09089813 0.09089813\n",
      " 0.09089813 0.09089813 0.09089813 0.09089813 0.09089813]\n",
      "Step 13 (67) @ Episode 8/10, action 1, reward 1,loss: 2.23592185974\n",
      "[0.09089796 0.09089796 0.09089796 0.09102036 0.09089796 0.09089796\n",
      " 0.09089796 0.09089796 0.09089796 0.09089796 0.09089796]\n",
      "Step 14 (68) @ Episode 8/10, action 3, reward 1,loss: 1.47316050529\n",
      "[0.0908978 0.0908978 0.0908978 0.091022  0.0908978 0.0908978 0.0908978\n",
      " 0.0908978 0.0908978 0.0908978 0.0908978]\n",
      "Step 15 (69) @ Episode 8/10, action 4, reward 1,loss: 1.46350812912\n",
      "[0.09089764 0.09089764 0.09089764 0.09102364 0.09089764 0.09089764\n",
      " 0.09089764 0.09089764 0.09089764 0.09089764 0.09089764]\n",
      "Step 16 (70) @ Episode 8/10, action 6, reward -1,loss: 1.5012717247\n",
      "[0.09089747 0.09089747 0.09089747 0.09102527 0.09089747 0.09089747\n",
      " 0.09089747 0.09089747 0.09089747 0.09089747 0.09089747]\n",
      "Step 17 (71) @ Episode 8/10, action 1, reward 1,loss: 1.47032785416\n",
      "[0.09089731 0.09089731 0.09089731 0.09089731 0.09089731 0.09089731\n",
      " 0.09089731 0.09089731 0.09089731 0.09102691 0.09089731]\n",
      "Step 18 (72) @ Episode 8/10, action 10, reward -3,loss: 1.7710814476\n",
      "Episode Reward: -3 Episode Length: 19\n",
      "[0.09089715 0.09089715 0.09089715 0.09102855 0.09089715 0.09089715\n",
      " 0.09089715 0.09089715 0.09089715 0.09089715 0.09089715]\n",
      "Step 0 (73) @ Episode 9/10, action 6, reward 1,loss: 1.80457401276\n",
      "[0.09089698 0.09089698 0.09089698 0.09103018 0.09089698 0.09089698\n",
      " 0.09089698 0.09089698 0.09089698 0.09089698 0.09089698]\n",
      "Step 1 (74) @ Episode 9/10, action 1, reward -1,loss: 1.48442840576\n",
      "[0.09089682 0.09089682 0.09089682 0.09103182 0.09089682 0.09089682\n",
      " 0.09089682 0.09089682 0.09089682 0.09089682 0.09089682]\n",
      "Step 2 (75) @ Episode 9/10, action 5, reward -1,loss: 1.70822930336\n",
      "[0.09089665 0.09089665 0.09089665 0.09103345 0.09089665 0.09089665\n",
      " 0.09089665 0.09089665 0.09089665 0.09089665 0.09089665]\n",
      "Step 3 (76) @ Episode 9/10, action 2, reward 1,loss: 1.2787361145\n",
      "[0.09089649 0.09089649 0.09103509 0.09089649 0.09089649 0.09089649\n",
      " 0.09089649 0.09089649 0.09089649 0.09089649 0.09089649]\n",
      "Step 4 (77) @ Episode 9/10, action 4, reward 1,loss: 1.48691296577\n",
      "[0.09089633 0.09089633 0.09089633 0.09103673 0.09089633 0.09089633\n",
      " 0.09089633 0.09089633 0.09089633 0.09089633 0.09089633]\n",
      "Step 5 (78) @ Episode 9/10, action 4, reward -1,loss: 1.98768424988\n",
      "[0.09089616 0.09089616 0.09089616 0.09103836 0.09089616 0.09089616\n",
      " 0.09089616 0.09089616 0.09089616 0.09089616 0.09089616]\n",
      "Step 6 (79) @ Episode 9/10, action 1, reward -1,loss: 2.25930070877\n",
      "[0.090896 0.090896 0.090896 0.09104  0.090896 0.090896 0.090896 0.090896\n",
      " 0.090896 0.090896 0.090896]\n",
      "Step 7 (80) @ Episode 9/10, action 1, reward -1,loss: 1.17991232872\n",
      "[0.09089584 0.09089584 0.09089584 0.09089584 0.09089584 0.09089584\n",
      " 0.09089584 0.09104164 0.09089584 0.09089584 0.09089584]\n",
      "Step 8 (81) @ Episode 9/10, action 3, reward -1,loss: 2.20890450478\n",
      "[0.09089567 0.09089567 0.09104327 0.09089567 0.09089567 0.09089567\n",
      " 0.09089567 0.09089567 0.09089567 0.09089567 0.09089567]\n",
      "Step 9 (82) @ Episode 9/10, action 6, reward 1,loss: 1.65679049492\n",
      "[0.09089551 0.09089551 0.09104491 0.09089551 0.09089551 0.09089551\n",
      " 0.09089551 0.09089551 0.09089551 0.09089551 0.09089551]\n",
      "Step 10 (83) @ Episode 9/10, action 2, reward -1,loss: 1.23130607605\n",
      "[0.09089535 0.09089535 0.09104655 0.09089535 0.09089535 0.09089535\n",
      " 0.09089535 0.09089535 0.09089535 0.09089535 0.09089535]\n",
      "Step 11 (84) @ Episode 9/10, action 5, reward -1,loss: 1.17352104187\n",
      "[0.09089518 0.09089518 0.09104818 0.09089518 0.09089518 0.09089518\n",
      " 0.09089518 0.09089518 0.09089518 0.09089518 0.09089518]\n",
      "Step 12 (85) @ Episode 9/10, action 10, reward 3,loss: 1.92013406754\n",
      "Episode Reward: -1 Episode Length: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09089502 0.09089502 0.09089502 0.09104982 0.09089502 0.09089502\n",
      " 0.09089502 0.09089502 0.09089502 0.09089502 0.09089502]\n",
      "Step 0 (86) @ Episode 10/10, action 6, reward 1,loss: 1.19502651691\n",
      "[0.09089485 0.09089485 0.09089485 0.09105145 0.09089485 0.09089485\n",
      " 0.09089485 0.09089485 0.09089485 0.09089485 0.09089485]\n",
      "Step 1 (87) @ Episode 10/10, action 10, reward 3,loss: 1.71290516853\n",
      "Episode Reward: 4 Episode Length: 2\n",
      "New image is being loaded: 2008_005337.jpg\n",
      "[0.09089469 0.09089469 0.09089469 0.09089469 0.09089469 0.09089469\n",
      " 0.09089469 0.09105309 0.09089469 0.09089469 0.09089469]\n",
      "Step 0 (88) @ Episode 1/10, action 4, reward 1,loss: 2.52299022675\n",
      "[0.09089453 0.09089453 0.09089453 0.09089453 0.09089453 0.09089453\n",
      " 0.09089453 0.09105473 0.09089453 0.09089453 0.09089453]\n",
      "Step 1 (89) @ Episode 1/10, action 7, reward -1,loss: 2.20088791847\n",
      "[0.09089436 0.09089436 0.09089436 0.09089436 0.09089436 0.09089436\n",
      " 0.09089436 0.09105636 0.09089436 0.09089436 0.09089436]\n",
      "Step 2 (90) @ Episode 1/10, action 3, reward -1,loss: 2.21938729286\n",
      "[0.0908942 0.0908942 0.0908942 0.0908942 0.0908942 0.0908942 0.0908942\n",
      " 0.091058  0.0908942 0.0908942 0.0908942]\n",
      "Step 3 (91) @ Episode 1/10, action 7, reward -1,loss: 1.18343317509\n",
      "[0.09089404 0.09089404 0.09089404 0.09089404 0.09089404 0.09089404\n",
      " 0.09089404 0.09105964 0.09089404 0.09089404 0.09089404]\n",
      "Step 4 (92) @ Episode 1/10, action 0, reward 1,loss: 1.17090606689\n",
      "[0.09089387 0.09089387 0.09089387 0.09089387 0.09089387 0.09089387\n",
      " 0.09089387 0.09106127 0.09089387 0.09089387 0.09089387]\n",
      "Step 5 (93) @ Episode 1/10, action 6, reward 1,loss: 2.23333930969\n",
      "[0.09089371 0.09089371 0.09089371 0.09089371 0.09089371 0.09089371\n",
      " 0.09089371 0.09106291 0.09089371 0.09089371 0.09089371]\n",
      "Step 6 (94) @ Episode 1/10, action 2, reward -1,loss: 1.73122417927\n",
      "[0.09089355 0.09089355 0.09089355 0.09106455 0.09089355 0.09089355\n",
      " 0.09089355 0.09089355 0.09089355 0.09089355 0.09089355]\n",
      "Step 7 (95) @ Episode 1/10, action 4, reward -1,loss: 2.51092767715\n",
      "[0.09089338 0.09089338 0.09106618 0.09089338 0.09089338 0.09089338\n",
      " 0.09089338 0.09089338 0.09089338 0.09089338 0.09089338]\n",
      "Step 8 (96) @ Episode 1/10, action 7, reward -1,loss: 1.21948325634\n",
      "[0.09089322 0.09089322 0.09089322 0.09106782 0.09089322 0.09089322\n",
      " 0.09089322 0.09089322 0.09089322 0.09089322 0.09089322]\n",
      "Step 9 (97) @ Episode 1/10, action 2, reward -1,loss: 3.21979093552\n",
      "[0.09089305 0.09089305 0.09089305 0.09106945 0.09089305 0.09089305\n",
      " 0.09089305 0.09089305 0.09089305 0.09089305 0.09089305]\n",
      "Step 10 (98) @ Episode 1/10, action 5, reward -1,loss: 1.4642803669\n",
      "[0.09089289 0.09089289 0.09089289 0.09107109 0.09089289 0.09089289\n",
      " 0.09089289 0.09089289 0.09089289 0.09089289 0.09089289]\n",
      "Step 11 (99) @ Episode 1/10, action 1, reward -1,loss: 2.4762775898\n",
      "[0.09089273 0.09089273 0.09089273 0.09107273 0.09089273 0.09089273\n",
      " 0.09089273 0.09089273 0.09089273 0.09089273 0.09089273]\n",
      "Step 12 (100) @ Episode 1/10, action 8, reward 1,loss: 1.00033247471\n",
      "[0.09089256 0.09089256 0.09089256 0.09107436 0.09089256 0.09089256\n",
      " 0.09089256 0.09089256 0.09089256 0.09089256 0.09089256]\n",
      "Step 13 (101) @ Episode 1/10, action 3, reward -1,loss: 2.23764586449\n",
      "[0.0908924 0.0908924 0.0908924 0.091076  0.0908924 0.0908924 0.0908924\n",
      " 0.0908924 0.0908924 0.0908924 0.0908924]\n",
      "Step 14 (102) @ Episode 1/10, action 3, reward -1,loss: 1.5192694664\n",
      "[0.09089224 0.09089224 0.09089224 0.09089224 0.09089224 0.09089224\n",
      " 0.09089224 0.09107764 0.09089224 0.09089224 0.09089224]\n",
      "Step 15 (103) @ Episode 1/10, action 7, reward 1,loss: 1.98455166817\n",
      "[0.09089207 0.09089207 0.09089207 0.09089207 0.09089207 0.09089207\n",
      " 0.09089207 0.09107927 0.09089207 0.09089207 0.09089207]\n",
      "Step 16 (104) @ Episode 1/10, action 7, reward -1,loss: 1.22505450249\n",
      "[0.09089191 0.09089191 0.09089191 0.09089191 0.09089191 0.09089191\n",
      " 0.09089191 0.09108091 0.09089191 0.09089191 0.09089191]\n",
      "Step 17 (105) @ Episode 1/10, action 7, reward -1,loss: 1.01113164425\n",
      "[0.09089175 0.09089175 0.09089175 0.09089175 0.09089175 0.09089175\n",
      " 0.09089175 0.09108255 0.09089175 0.09089175 0.09089175]\n",
      "Step 18 (106) @ Episode 1/10, action 9, reward 1,loss: 1.93159413338\n",
      "[0.09089158 0.09089158 0.09089158 0.09089158 0.09089158 0.09089158\n",
      " 0.09089158 0.09108418 0.09089158 0.09089158 0.09089158]\n",
      "Step 19 (107) @ Episode 1/10, action 9, reward -1,loss: 1.75699329376\n",
      "[0.09089142 0.09089142 0.09089142 0.09108582 0.09089142 0.09089142\n",
      " 0.09089142 0.09089142 0.09089142 0.09089142 0.09089142]\n",
      "Step 20 (108) @ Episode 1/10, action 6, reward -1,loss: 1.72545421124\n",
      "[0.09089125 0.09089125 0.09089125 0.09089125 0.09089125 0.09089125\n",
      " 0.09089125 0.09089125 0.09089125 0.09089125 0.09108745]\n",
      "Step 21 (109) @ Episode 1/10, action 5, reward -1,loss: 1.51706373692\n",
      "[0.09089109 0.09089109 0.09089109 0.09108909 0.09089109 0.09089109\n",
      " 0.09089109 0.09089109 0.09089109 0.09089109 0.09089109]\n",
      "Step 22 (110) @ Episode 1/10, action 10, reward -3,loss: 2.24135494232\n",
      "Episode Reward: -13 Episode Length: 23\n",
      "[0.09089093 0.09089093 0.09089093 0.09089093 0.09089093 0.09089093\n",
      " 0.09089093 0.09109073 0.09089093 0.09089093 0.09089093]\n",
      "Step 0 (111) @ Episode 2/10, action 6, reward 1,loss: 1.48316574097\n",
      "[0.09089076 0.09089076 0.09089076 0.09089076 0.09089076 0.09089076\n",
      " 0.09089076 0.09109236 0.09089076 0.09089076 0.09089076]\n",
      "Step 1 (112) @ Episode 2/10, action 7, reward -1,loss: 2.74427843094\n",
      "[0.0908906 0.0908906 0.0908906 0.091094  0.0908906 0.0908906 0.0908906\n",
      " 0.0908906 0.0908906 0.0908906 0.0908906]\n",
      "Step 2 (113) @ Episode 2/10, action 5, reward -1,loss: 1.65950143337\n",
      "[0.09089044 0.09089044 0.09089044 0.09089044 0.09089044 0.09089044\n",
      " 0.09089044 0.09109564 0.09089044 0.09089044 0.09089044]\n",
      "Step 3 (114) @ Episode 2/10, action 5, reward -1,loss: 1.46238589287\n",
      "[0.09089027 0.09089027 0.09089027 0.09109727 0.09089027 0.09089027\n",
      " 0.09089027 0.09089027 0.09089027 0.09089027 0.09089027]\n",
      "Step 4 (115) @ Episode 2/10, action 7, reward -1,loss: 2.17909240723\n",
      "[0.09089011 0.09089011 0.09089011 0.09109891 0.09089011 0.09089011\n",
      " 0.09089011 0.09089011 0.09089011 0.09089011 0.09089011]\n",
      "Step 5 (116) @ Episode 2/10, action 1, reward -1,loss: 1.19567012787\n",
      "[0.09088995 0.09088995 0.09088995 0.09110055 0.09088995 0.09088995\n",
      " 0.09088995 0.09088995 0.09088995 0.09088995 0.09088995]\n",
      "Step 6 (117) @ Episode 2/10, action 8, reward 1,loss: 1.45856559277\n",
      "[0.09088978 0.09088978 0.09088978 0.09110218 0.09088978 0.09088978\n",
      " 0.09088978 0.09088978 0.09088978 0.09088978 0.09088978]\n",
      "Step 7 (118) @ Episode 2/10, action 6, reward 1,loss: 1.71920967102\n",
      "[0.09088962 0.09088962 0.09088962 0.09088962 0.09088962 0.09088962\n",
      " 0.09088962 0.09110382 0.09088962 0.09088962 0.09088962]\n",
      "Step 8 (119) @ Episode 2/10, action 5, reward 1,loss: 1.21365094185\n",
      "[0.09088945 0.09088945 0.09088945 0.09088945 0.09088945 0.09088945\n",
      " 0.09088945 0.09110545 0.09088945 0.09088945 0.09088945]\n",
      "Step 9 (120) @ Episode 2/10, action 5, reward -1,loss: 1.50838327408\n",
      "[0.09088929 0.09088929 0.09088929 0.09088929 0.09088929 0.09088929\n",
      " 0.09088929 0.09110709 0.09088929 0.09088929 0.09088929]\n",
      "Step 10 (121) @ Episode 2/10, action 5, reward -1,loss: 2.14153623581\n",
      "[0.09088913 0.09088913 0.09088913 0.09088913 0.09088913 0.09088913\n",
      " 0.09088913 0.09110873 0.09088913 0.09088913 0.09088913]\n",
      "Step 11 (122) @ Episode 2/10, action 5, reward -1,loss: 1.44772672653\n",
      "[0.09088896 0.09088896 0.09088896 0.09088896 0.09088896 0.09088896\n",
      " 0.09088896 0.09088896 0.09088896 0.09088896 0.09111036]\n",
      "Step 12 (123) @ Episode 2/10, action 1, reward 1,loss: 1.96759307384\n",
      "[0.0908888 0.0908888 0.0908888 0.0908888 0.0908888 0.0908888 0.0908888\n",
      " 0.091112  0.0908888 0.0908888 0.0908888]\n",
      "Step 13 (124) @ Episode 2/10, action 6, reward 1,loss: 1.65053462982\n",
      "[0.09088864 0.09088864 0.09088864 0.09088864 0.09088864 0.09088864\n",
      " 0.09088864 0.09111364 0.09088864 0.09088864 0.09088864]\n",
      "Step 14 (125) @ Episode 2/10, action 9, reward -1,loss: 1.45280504227\n",
      "[0.09088847 0.09088847 0.09088847 0.09088847 0.09088847 0.09088847\n",
      " 0.09088847 0.09111527 0.09088847 0.09088847 0.09088847]\n",
      "Step 15 (126) @ Episode 2/10, action 7, reward 1,loss: 1.6459338665\n",
      "[0.09088831 0.09088831 0.09088831 0.09088831 0.09088831 0.09088831\n",
      " 0.09088831 0.09111691 0.09088831 0.09088831 0.09088831]\n",
      "Step 16 (127) @ Episode 2/10, action 4, reward -1,loss: 1.66208708286\n",
      "[0.09088815 0.09088815 0.09088815 0.09088815 0.09088815 0.09088815\n",
      " 0.09088815 0.09111855 0.09088815 0.09088815 0.09088815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17 (128) @ Episode 2/10, action 2, reward 1,loss: 2.24805784225\n",
      "[0.09088798 0.09088798 0.09088798 0.09112018 0.09088798 0.09088798\n",
      " 0.09088798 0.09088798 0.09088798 0.09088798 0.09088798]\n",
      "Step 18 (129) @ Episode 2/10, action 1, reward 1,loss: 1.45920205116\n",
      "[0.09088782 0.09088782 0.09088782 0.09112182 0.09088782 0.09088782\n",
      " 0.09088782 0.09088782 0.09088782 0.09088782 0.09088782]\n",
      "Step 19 (130) @ Episode 2/10, action 10, reward -3,loss: 1.92299151421\n",
      "Episode Reward: -4 Episode Length: 20\n",
      "[0.09088765 0.09088765 0.09088765 0.09088765 0.09088765 0.09088765\n",
      " 0.09088765 0.09112345 0.09088765 0.09088765 0.09088765]\n",
      "Step 0 (131) @ Episode 3/10, action 8, reward 1,loss: 1.4802043438\n",
      "[0.09088749 0.09088749 0.09088749 0.09088749 0.09088749 0.09088749\n",
      " 0.09088749 0.09112509 0.09088749 0.09088749 0.09088749]\n",
      "Step 1 (132) @ Episode 3/10, action 1, reward 1,loss: 2.46105766296\n",
      "[0.09088733 0.09088733 0.09088733 0.09088733 0.09088733 0.09088733\n",
      " 0.09088733 0.09112673 0.09088733 0.09088733 0.09088733]\n",
      "Step 2 (133) @ Episode 3/10, action 8, reward -1,loss: 1.49209368229\n",
      "[0.09088716 0.09088716 0.09088716 0.09088716 0.09088716 0.09088716\n",
      " 0.09088716 0.09112836 0.09088716 0.09088716 0.09088716]\n",
      "Step 3 (134) @ Episode 3/10, action 6, reward -1,loss: 1.6760263443\n",
      "[0.090887 0.090887 0.090887 0.090887 0.090887 0.090887 0.090887 0.09113\n",
      " 0.090887 0.090887 0.090887]\n",
      "Step 4 (135) @ Episode 3/10, action 2, reward 1,loss: 1.64359617233\n",
      "[0.09088684 0.09088684 0.09088684 0.09113164 0.09088684 0.09088684\n",
      " 0.09088684 0.09088684 0.09088684 0.09088684 0.09088684]\n",
      "Step 5 (136) @ Episode 3/10, action 5, reward -1,loss: 1.46723353863\n",
      "[0.09088667 0.09088667 0.09088667 0.09113327 0.09088667 0.09088667\n",
      " 0.09088667 0.09088667 0.09088667 0.09088667 0.09088667]\n",
      "Step 6 (137) @ Episode 3/10, action 8, reward -1,loss: 0.933739721775\n",
      "[0.09088651 0.09088651 0.09088651 0.09088651 0.09088651 0.09088651\n",
      " 0.09088651 0.09113491 0.09088651 0.09088651 0.09088651]\n",
      "Step 7 (138) @ Episode 3/10, action 7, reward -1,loss: 1.73205113411\n",
      "[0.09088635 0.09088635 0.09088635 0.09113655 0.09088635 0.09088635\n",
      " 0.09088635 0.09088635 0.09088635 0.09088635 0.09088635]\n",
      "Step 8 (139) @ Episode 3/10, action 4, reward -1,loss: 1.9574867487\n",
      "[0.09088618 0.09088618 0.09088618 0.09113818 0.09088618 0.09088618\n",
      " 0.09088618 0.09088618 0.09088618 0.09088618 0.09088618]\n",
      "Step 9 (140) @ Episode 3/10, action 5, reward -1,loss: 1.41299569607\n",
      "[0.09088602 0.09088602 0.09088602 0.09088602 0.09088602 0.09088602\n",
      " 0.09088602 0.09113982 0.09088602 0.09088602 0.09088602]\n",
      "Step 10 (141) @ Episode 3/10, action 6, reward -1,loss: 1.97723019123\n",
      "[0.09088585 0.09088585 0.09088585 0.09088585 0.09088585 0.09088585\n",
      " 0.09088585 0.09114146 0.09088585 0.09088585 0.09088585]\n",
      "Step 11 (142) @ Episode 3/10, action 0, reward -1,loss: 1.68420171738\n",
      "[0.09088569 0.09088569 0.09088569 0.09088569 0.09088569 0.09088569\n",
      " 0.09088569 0.09114309 0.09088569 0.09088569 0.09088569]\n",
      "Step 12 (143) @ Episode 3/10, action 9, reward -1,loss: 2.2389421463\n",
      "[0.09088553 0.09088553 0.09088553 0.09088553 0.09088553 0.09088553\n",
      " 0.09088553 0.09114473 0.09088553 0.09088553 0.09088553]\n",
      "Step 13 (144) @ Episode 3/10, action 8, reward -1,loss: 1.71404290199\n",
      "[0.09088536 0.09088536 0.09088536 0.09088536 0.09088536 0.09088536\n",
      " 0.09088536 0.09114636 0.09088536 0.09088536 0.09088536]\n",
      "Step 14 (145) @ Episode 3/10, action 4, reward -1,loss: 0.932331502438\n",
      "[0.0908852 0.0908852 0.0908852 0.091148  0.0908852 0.0908852 0.0908852\n",
      " 0.0908852 0.0908852 0.0908852 0.0908852]\n",
      "Step 15 (146) @ Episode 3/10, action 5, reward -1,loss: 1.72214007378\n",
      "[0.09088504 0.09088504 0.09088504 0.09088504 0.09088504 0.09088504\n",
      " 0.09088504 0.09114964 0.09088504 0.09088504 0.09088504]\n",
      "Step 16 (147) @ Episode 3/10, action 4, reward -1,loss: 2.34851956367\n",
      "[0.09088487 0.09088487 0.09088487 0.09088487 0.09088487 0.09088487\n",
      " 0.09088487 0.09115127 0.09088487 0.09088487 0.09088487]\n",
      "Step 17 (148) @ Episode 3/10, action 10, reward -3,loss: 1.48522853851\n",
      "Episode Reward: -14 Episode Length: 18\n",
      "[0.09088471 0.09088471 0.09088471 0.09088471 0.09088471 0.09088471\n",
      " 0.09088471 0.09115291 0.09088471 0.09088471 0.09088471]\n",
      "Step 0 (149) @ Episode 4/10, action 0, reward 1,loss: 1.70814549923\n",
      "[0.09088455 0.09088455 0.09088455 0.09088455 0.09088455 0.09088455\n",
      " 0.09088455 0.09115455 0.09088455 0.09088455 0.09088455]\n",
      "Step 1 (150) @ Episode 4/10, action 3, reward -1,loss: 1.91466152668\n",
      "[0.09088438 0.09088438 0.09088438 0.09088438 0.09088438 0.09088438\n",
      " 0.09088438 0.09115618 0.09088438 0.09088438 0.09088438]\n",
      "Step 2 (151) @ Episode 4/10, action 10, reward -3,loss: 1.14466905594\n",
      "Episode Reward: -3 Episode Length: 3\n",
      "[0.09088422 0.09088422 0.09088422 0.09088422 0.09088422 0.09088422\n",
      " 0.09088422 0.09115782 0.09088422 0.09088422 0.09088422]\n",
      "Step 0 (152) @ Episode 5/10, action 0, reward 1,loss: 1.47761249542\n",
      "[0.09088405 0.09088405 0.09088405 0.09088405 0.09088405 0.09088405\n",
      " 0.09088405 0.09115946 0.09088405 0.09088405 0.09088405]\n",
      "Step 1 (153) @ Episode 5/10, action 4, reward -1,loss: 1.71025598049\n",
      "[0.09088389 0.09088389 0.09088389 0.09088389 0.09088389 0.09088389\n",
      " 0.09088389 0.09116109 0.09088389 0.09088389 0.09088389]\n",
      "Step 2 (154) @ Episode 5/10, action 10, reward -3,loss: 2.15352869034\n",
      "Episode Reward: -3 Episode Length: 3\n",
      "[0.09088373 0.09088373 0.09088373 0.09088373 0.09088373 0.09088373\n",
      " 0.09088373 0.09116273 0.09088373 0.09088373 0.09088373]\n",
      "Step 0 (155) @ Episode 6/10, action 7, reward 1,loss: 1.42066967487\n",
      "[0.09088356 0.09088356 0.09088356 0.09088356 0.09088356 0.09088356\n",
      " 0.09088356 0.09116436 0.09088356 0.09088356 0.09088356]\n",
      "Step 1 (156) @ Episode 6/10, action 0, reward 1,loss: 0.958806872368\n",
      "[0.0908834 0.0908834 0.0908834 0.0908834 0.0908834 0.0908834 0.0908834\n",
      " 0.091166  0.0908834 0.0908834 0.0908834]\n",
      "Step 2 (157) @ Episode 6/10, action 10, reward -3,loss: 1.14880228043\n",
      "Episode Reward: -1 Episode Length: 3\n",
      "[0.09088324 0.09088324 0.09088324 0.09088324 0.09088324 0.09088324\n",
      " 0.09088324 0.09116764 0.09088324 0.09088324 0.09088324]\n",
      "Step 0 (158) @ Episode 7/10, action 2, reward 1,loss: 1.64822816849\n",
      "[0.09088307 0.09088307 0.09088307 0.09088307 0.09088307 0.09088307\n",
      " 0.09088307 0.09116927 0.09088307 0.09088307 0.09088307]\n",
      "Step 1 (159) @ Episode 7/10, action 4, reward -1,loss: 1.6336183548\n",
      "[0.09088291 0.09088291 0.09088291 0.09088291 0.09088291 0.09088291\n",
      " 0.09088291 0.09117091 0.09088291 0.09088291 0.09088291]\n",
      "Step 2 (160) @ Episode 7/10, action 7, reward -1,loss: 2.15338754654\n",
      "[0.09088275 0.09088275 0.09088275 0.09088275 0.09088275 0.09088275\n",
      " 0.09088275 0.09117255 0.09088275 0.09088275 0.09088275]\n",
      "Step 3 (161) @ Episode 7/10, action 2, reward -1,loss: 2.14851617813\n",
      "[0.09088258 0.09088258 0.09088258 0.09088258 0.09088258 0.09088258\n",
      " 0.09088258 0.09117418 0.09088258 0.09088258 0.09088258]\n",
      "Step 4 (162) @ Episode 7/10, action 9, reward 1,loss: 2.19410657883\n",
      "[0.09088242 0.09088242 0.09088242 0.09088242 0.09088242 0.09088242\n",
      " 0.09088242 0.09117582 0.09088242 0.09088242 0.09088242]\n",
      "Step 5 (163) @ Episode 7/10, action 9, reward -1,loss: 1.66714286804\n",
      "[0.09088225 0.09088225 0.09088225 0.09088225 0.09088225 0.09088225\n",
      " 0.09088225 0.09117746 0.09088225 0.09088225 0.09088225]\n",
      "Step 6 (164) @ Episode 7/10, action 3, reward 1,loss: 1.87071442604\n",
      "[0.09088209 0.09088209 0.09088209 0.09088209 0.09088209 0.09088209\n",
      " 0.09088209 0.09117909 0.09088209 0.09088209 0.09088209]\n",
      "Step 7 (165) @ Episode 7/10, action 2, reward -1,loss: 1.66672539711\n",
      "[0.09088193 0.09088193 0.09088193 0.09088193 0.09088193 0.09088193\n",
      " 0.09088193 0.09088193 0.09088193 0.09088193 0.09118073]\n",
      "Step 8 (166) @ Episode 7/10, action 7, reward -1,loss: 1.45367884636\n",
      "[0.09088176 0.09118236 0.09088176 0.09088176 0.09088176 0.09088176\n",
      " 0.09088176 0.09088176 0.09088176 0.09088176 0.09088176]\n",
      "Step 9 (167) @ Episode 7/10, action 4, reward -1,loss: 1.64358592033\n",
      "[0.0908816 0.091184  0.0908816 0.0908816 0.0908816 0.0908816 0.0908816\n",
      " 0.0908816 0.0908816 0.0908816 0.0908816]\n",
      "Step 10 (168) @ Episode 7/10, action 1, reward 1,loss: 1.93685793877\n",
      "[0.09088144 0.09088144 0.09088144 0.09088144 0.09088144 0.09088144\n",
      " 0.09088144 0.09118564 0.09088144 0.09088144 0.09088144]\n",
      "Step 11 (169) @ Episode 7/10, action 5, reward -1,loss: 1.53297281265\n",
      "[0.09088127 0.09088127 0.09088127 0.09088127 0.09088127 0.09088127\n",
      " 0.09088127 0.09118727 0.09088127 0.09088127 0.09088127]\n",
      "Step 12 (170) @ Episode 7/10, action 10, reward -3,loss: 1.73981511593\n",
      "Episode Reward: -7 Episode Length: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09088111 0.09088111 0.09088111 0.09088111 0.09088111 0.09088111\n",
      " 0.09088111 0.09118891 0.09088111 0.09088111 0.09088111]\n",
      "Step 0 (171) @ Episode 8/10, action 8, reward 1,loss: 1.67187571526\n",
      "[0.09088095 0.09088095 0.09088095 0.09088095 0.09088095 0.09088095\n",
      " 0.09088095 0.09119055 0.09088095 0.09088095 0.09088095]\n",
      "Step 1 (172) @ Episode 8/10, action 2, reward -1,loss: 1.70780444145\n",
      "[0.09088078 0.09088078 0.09088078 0.09088078 0.09088078 0.09088078\n",
      " 0.09088078 0.09119218 0.09088078 0.09088078 0.09088078]\n",
      "Step 2 (173) @ Episode 8/10, action 10, reward -3,loss: 1.71900439262\n",
      "Episode Reward: -3 Episode Length: 3\n",
      "[0.09088062 0.09088062 0.09088062 0.09088062 0.09088062 0.09088062\n",
      " 0.09088062 0.09119382 0.09088062 0.09088062 0.09088062]\n",
      "Step 0 (174) @ Episode 9/10, action 9, reward 1,loss: 2.45848345757\n",
      "[0.09088045 0.09088045 0.09088045 0.09088045 0.09088045 0.09088045\n",
      " 0.09088045 0.09119546 0.09088045 0.09088045 0.09088045]\n",
      "Step 1 (175) @ Episode 9/10, action 6, reward 1,loss: 1.61372029781\n",
      "[0.09088029 0.09088029 0.09088029 0.09088029 0.09088029 0.09088029\n",
      " 0.09088029 0.09119709 0.09088029 0.09088029 0.09088029]\n",
      "Step 2 (176) @ Episode 9/10, action 10, reward -3,loss: 1.44611978531\n",
      "Episode Reward: -1 Episode Length: 3\n",
      "[0.09088013 0.09088013 0.09088013 0.09088013 0.09088013 0.09088013\n",
      " 0.09088013 0.09119873 0.09088013 0.09088013 0.09088013]\n",
      "Step 0 (177) @ Episode 10/10, action 4, reward 1,loss: 1.9593667984\n",
      "[0.09087996 0.09087996 0.09087996 0.09087996 0.09087996 0.09087996\n",
      " 0.09087996 0.09120036 0.09087996 0.09087996 0.09087996]\n",
      "Step 1 (178) @ Episode 10/10, action 5, reward -1,loss: 1.67047429085\n",
      "[0.0908798 0.0908798 0.0908798 0.0908798 0.0908798 0.0908798 0.0908798\n",
      " 0.091202  0.0908798 0.0908798 0.0908798]\n",
      "Step 2 (179) @ Episode 10/10, action 5, reward -1,loss: 1.91924965382\n",
      "[0.09087964 0.09087964 0.09087964 0.09087964 0.09087964 0.09087964\n",
      " 0.09087964 0.09120364 0.09087964 0.09087964 0.09087964]\n",
      "Step 3 (180) @ Episode 10/10, action 5, reward -1,loss: 1.92932581902\n",
      "[0.09087947 0.09087947 0.09087947 0.09087947 0.09087947 0.09087947\n",
      " 0.09087947 0.09120527 0.09087947 0.09087947 0.09087947]\n",
      "Step 4 (181) @ Episode 10/10, action 3, reward -1,loss: 2.3956990242\n",
      "[0.09087931 0.09087931 0.09087931 0.09087931 0.09087931 0.09087931\n",
      " 0.09087931 0.09120691 0.09087931 0.09087931 0.09087931]\n",
      "Step 5 (182) @ Episode 10/10, action 6, reward 1,loss: 1.18439865112\n",
      "[0.09087915 0.09087915 0.09087915 0.09087915 0.09087915 0.09087915\n",
      " 0.09087915 0.09120855 0.09087915 0.09087915 0.09087915]\n",
      "Step 6 (183) @ Episode 10/10, action 5, reward -1,loss: 2.20614051819\n",
      "[0.09087898 0.09087898 0.09087898 0.09087898 0.09087898 0.09087898\n",
      " 0.09087898 0.09121018 0.09087898 0.09087898 0.09087898]\n",
      "Step 7 (184) @ Episode 10/10, action 9, reward -1,loss: 2.199634552\n",
      "[0.09087882 0.09087882 0.09087882 0.09087882 0.09087882 0.09087882\n",
      " 0.09087882 0.09121182 0.09087882 0.09087882 0.09087882]\n",
      "Step 8 (185) @ Episode 10/10, action 5, reward -1,loss: 2.14087629318\n",
      "[0.09087865 0.09087865 0.09087865 0.09121346 0.09087865 0.09087865\n",
      " 0.09087865 0.09087865 0.09087865 0.09087865 0.09087865]\n",
      "Step 9 (186) @ Episode 10/10, action 10, reward -3,loss: 1.97194314003\n",
      "Episode Reward: -8 Episode Length: 10\n",
      "New image is being loaded: 2011_000855.jpg\n",
      "[0.09087849 0.09087849 0.09087849 0.09087849 0.09087849 0.09087849\n",
      " 0.09087849 0.09121509 0.09087849 0.09087849 0.09087849]\n",
      "Step 0 (187) @ Episode 1/10, action 10, reward 3,loss: 2.11282134056\n",
      "Episode Reward: 3 Episode Length: 1\n",
      "[0.09087833 0.09087833 0.09087833 0.09087833 0.09087833 0.09087833\n",
      " 0.09087833 0.09121673 0.09087833 0.09087833 0.09087833]\n",
      "Step 0 (188) @ Episode 2/10, action 9, reward 1,loss: 1.43580341339\n",
      "[0.09087816 0.09087816 0.09087816 0.09087816 0.09087816 0.09087816\n",
      " 0.09087816 0.09121836 0.09087816 0.09087816 0.09087816]\n",
      "Step 1 (189) @ Episode 2/10, action 1, reward 1,loss: 1.6701271534\n",
      "[0.090878 0.090878 0.090878 0.090878 0.090878 0.090878 0.090878 0.09122\n",
      " 0.090878 0.090878 0.090878]\n",
      "Step 2 (190) @ Episode 2/10, action 2, reward -1,loss: 1.61747050285\n",
      "[0.09087784 0.09087784 0.09087784 0.09122164 0.09087784 0.09087784\n",
      " 0.09087784 0.09087784 0.09087784 0.09087784 0.09087784]\n",
      "Step 3 (191) @ Episode 2/10, action 3, reward 1,loss: 2.24353885651\n",
      "[0.09087767 0.09122327 0.09087767 0.09087767 0.09087767 0.09087767\n",
      " 0.09087767 0.09087767 0.09087767 0.09087767 0.09087767]\n",
      "Step 4 (192) @ Episode 2/10, action 5, reward -1,loss: 1.6086127758\n",
      "[0.09087751 0.09087751 0.09122491 0.09087751 0.09087751 0.09087751\n",
      " 0.09087751 0.09087751 0.09087751 0.09087751 0.09087751]\n",
      "Step 5 (193) @ Episode 2/10, action 7, reward -1,loss: 1.44384968281\n",
      "[0.09087735 0.09087735 0.09087735 0.09087735 0.09087735 0.09087735\n",
      " 0.09087735 0.09122655 0.09087735 0.09087735 0.09087735]\n",
      "Step 6 (194) @ Episode 2/10, action 2, reward -1,loss: 1.72144210339\n",
      "[0.09087718 0.09087718 0.09087718 0.09087718 0.09087718 0.09087718\n",
      " 0.09087718 0.09122818 0.09087718 0.09087718 0.09087718]\n",
      "Step 7 (195) @ Episode 2/10, action 8, reward -1,loss: 2.15045547485\n",
      "[0.09087702 0.09087702 0.09087702 0.09122982 0.09087702 0.09087702\n",
      " 0.09087702 0.09087702 0.09087702 0.09087702 0.09087702]\n",
      "Step 8 (196) @ Episode 2/10, action 3, reward 1,loss: 1.6610352993\n",
      "[0.09087685 0.09087685 0.09087685 0.09123146 0.09087685 0.09087685\n",
      " 0.09087685 0.09087685 0.09087685 0.09087685 0.09087685]\n",
      "Step 9 (197) @ Episode 2/10, action 7, reward 1,loss: 0.904173433781\n",
      "[0.09087669 0.09087669 0.09087669 0.09087669 0.09087669 0.09087669\n",
      " 0.09087669 0.09123309 0.09087669 0.09087669 0.09087669]\n",
      "Step 10 (198) @ Episode 2/10, action 4, reward -1,loss: 1.42105460167\n",
      "[0.09087653 0.09123473 0.09087653 0.09087653 0.09087653 0.09087653\n",
      " 0.09087653 0.09087653 0.09087653 0.09087653 0.09087653]\n",
      "Step 11 (199) @ Episode 2/10, action 1, reward 1,loss: 1.23867964745\n",
      "[0.09087636 0.09087636 0.09087636 0.09087636 0.09087636 0.09087636\n",
      " 0.09087636 0.09123636 0.09087636 0.09087636 0.09087636]\n",
      "Step 12 (200) @ Episode 2/10, action 1, reward -1,loss: 1.72189235687\n",
      "[0.0908762 0.0908762 0.0908762 0.0908762 0.0908762 0.0908762 0.0908762\n",
      " 0.091238  0.0908762 0.0908762 0.0908762]\n",
      "Step 13 (201) @ Episode 2/10, action 0, reward 1,loss: 1.68527114391\n",
      "[0.09087604 0.09087604 0.09087604 0.09087604 0.09087604 0.09087604\n",
      " 0.09087604 0.09123964 0.09087604 0.09087604 0.09087604]\n",
      "Step 14 (202) @ Episode 2/10, action 1, reward -1,loss: 2.17091393471\n",
      "[0.09087587 0.09087587 0.09087587 0.09087587 0.09087587 0.09087587\n",
      " 0.09087587 0.09124127 0.09087587 0.09087587 0.09087587]\n",
      "Step 15 (203) @ Episode 2/10, action 4, reward -1,loss: 2.22711253166\n",
      "[0.09087571 0.09087571 0.09087571 0.09087571 0.09087571 0.09087571\n",
      " 0.09087571 0.09124291 0.09087571 0.09087571 0.09087571]\n",
      "Step 16 (204) @ Episode 2/10, action 4, reward -1,loss: 1.91788518429\n",
      "[0.09087555 0.09087555 0.09087555 0.09087555 0.09087555 0.09087555\n",
      " 0.09087555 0.09124455 0.09087555 0.09087555 0.09087555]\n",
      "Step 17 (205) @ Episode 2/10, action 0, reward 1,loss: 2.213940382\n",
      "[0.09087538 0.09087538 0.09087538 0.09087538 0.09087538 0.09087538\n",
      " 0.09087538 0.09124618 0.09087538 0.09087538 0.09087538]\n",
      "Step 18 (206) @ Episode 2/10, action 4, reward -1,loss: 1.64790415764\n",
      "[0.09087522 0.09087522 0.09087522 0.09087522 0.09087522 0.09087522\n",
      " 0.09087522 0.09124782 0.09087522 0.09087522 0.09087522]\n",
      "Step 19 (207) @ Episode 2/10, action 0, reward 1,loss: 1.66426157951\n",
      "[0.09087505 0.09087505 0.09087505 0.09087505 0.09087505 0.09087505\n",
      " 0.09087505 0.09124946 0.09087505 0.09087505 0.09087505]\n",
      "Step 20 (208) @ Episode 2/10, action 3, reward 1,loss: 1.45238614082\n",
      "[0.09087489 0.09087489 0.09087489 0.09087489 0.09087489 0.09087489\n",
      " 0.09087489 0.09125109 0.09087489 0.09087489 0.09087489]\n",
      "Step 21 (209) @ Episode 2/10, action 8, reward -1,loss: 1.4646654129\n",
      "[0.09087473 0.09087473 0.09087473 0.09087473 0.09087473 0.09087473\n",
      " 0.09087473 0.09125273 0.09087473 0.09087473 0.09087473]\n",
      "Step 22 (210) @ Episode 2/10, action 4, reward -1,loss: 1.85689866543\n",
      "[0.09087456 0.09087456 0.09087456 0.09087456 0.09087456 0.09087456\n",
      " 0.09087456 0.09125436 0.09087456 0.09087456 0.09087456]\n",
      "Step 23 (211) @ Episode 2/10, action 4, reward -1,loss: 1.96398055553\n",
      "[0.0908744 0.091256  0.0908744 0.0908744 0.0908744 0.0908744 0.0908744\n",
      " 0.0908744 0.0908744 0.0908744 0.0908744]\n",
      "Step 24 (212) @ Episode 2/10, action 1, reward 1,loss: 1.8989803791\n",
      "[0.09087424 0.09087424 0.09087424 0.09087424 0.09087424 0.09087424\n",
      " 0.09087424 0.09125764 0.09087424 0.09087424 0.09087424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25 (213) @ Episode 2/10, action 5, reward -1,loss: 1.64979124069\n",
      "[0.09087407 0.09087407 0.09087407 0.09087407 0.09087407 0.09087407\n",
      " 0.09087407 0.09125927 0.09087407 0.09087407 0.09087407]\n",
      "Step 26 (214) @ Episode 2/10, action 1, reward 1,loss: 1.90898442268\n",
      "[0.09087391 0.09087391 0.09087391 0.09087391 0.09087391 0.09087391\n",
      " 0.09087391 0.09126091 0.09087391 0.09087391 0.09087391]\n",
      "Step 27 (215) @ Episode 2/10, action 5, reward -1,loss: 3.11624288559\n",
      "[0.09087375 0.09087375 0.09087375 0.09087375 0.09087375 0.09087375\n",
      " 0.09087375 0.09126255 0.09087375 0.09087375 0.09087375]\n",
      "Step 28 (216) @ Episode 2/10, action 0, reward 1,loss: 2.08072042465\n",
      "[0.09087358 0.09087358 0.09087358 0.09087358 0.09087358 0.09087358\n",
      " 0.09087358 0.09126418 0.09087358 0.09087358 0.09087358]\n",
      "Step 29 (217) @ Episode 2/10, action 5, reward -1,loss: 1.87797033787\n",
      "[0.09087342 0.09087342 0.09087342 0.09087342 0.09087342 0.09087342\n",
      " 0.09087342 0.09126582 0.09087342 0.09087342 0.09087342]\n",
      "Step 30 (218) @ Episode 2/10, action 3, reward -1,loss: 1.66835904121\n",
      "[0.09087325 0.09087325 0.09087325 0.09087325 0.09087325 0.09087325\n",
      " 0.09087325 0.09126746 0.09087325 0.09087325 0.09087325]\n",
      "Step 31 (219) @ Episode 2/10, action 9, reward -1,loss: 1.20442914963\n",
      "[0.09087309 0.09087309 0.09087309 0.09087309 0.09087309 0.09087309\n",
      " 0.09087309 0.09126909 0.09087309 0.09087309 0.09087309]\n",
      "Step 32 (220) @ Episode 2/10, action 5, reward -1,loss: 1.61445820332\n",
      "[0.09087293 0.09087293 0.09087293 0.09087293 0.09087293 0.09087293\n",
      " 0.09087293 0.09127073 0.09087293 0.09087293 0.09087293]\n",
      "Step 33 (221) @ Episode 2/10, action 7, reward 1,loss: 1.9828568697\n",
      "[0.09087276 0.09087276 0.09087276 0.09087276 0.09087276 0.09087276\n",
      " 0.09087276 0.09127236 0.09087276 0.09087276 0.09087276]\n",
      "Step 34 (222) @ Episode 2/10, action 8, reward -1,loss: 1.19934821129\n",
      "[0.0908726 0.0908726 0.0908726 0.0908726 0.0908726 0.0908726 0.0908726\n",
      " 0.091274  0.0908726 0.0908726 0.0908726]\n",
      "Step 35 (223) @ Episode 2/10, action 5, reward -1,loss: 1.67890846729\n",
      "[0.09087244 0.09127564 0.09087244 0.09087244 0.09087244 0.09087244\n",
      " 0.09087244 0.09087244 0.09087244 0.09087244 0.09087244]\n",
      "Step 36 (224) @ Episode 2/10, action 0, reward 1,loss: 1.44551980495\n",
      "[0.09087227 0.09127727 0.09087227 0.09087227 0.09087227 0.09087227\n",
      " 0.09087227 0.09087227 0.09087227 0.09087227 0.09087227]\n",
      "Step 37 (225) @ Episode 2/10, action 3, reward 1,loss: 2.02060461044\n",
      "[0.09087211 0.09087211 0.09087211 0.09087211 0.09087211 0.09087211\n",
      " 0.09087211 0.09127891 0.09087211 0.09087211 0.09087211]\n",
      "Step 38 (226) @ Episode 2/10, action 9, reward -1,loss: 1.24112045765\n",
      "[0.09087195 0.09087195 0.09087195 0.09087195 0.09087195 0.09087195\n",
      " 0.09087195 0.09128055 0.09087195 0.09087195 0.09087195]\n",
      "Step 39 (227) @ Episode 2/10, action 5, reward -1,loss: 1.84361064434\n",
      "[0.09087178 0.09087178 0.09087178 0.09087178 0.09087178 0.09087178\n",
      " 0.09087178 0.09128218 0.09087178 0.09087178 0.09087178]\n",
      "Step 40 (228) @ Episode 2/10, action 2, reward 1,loss: 1.785192132\n",
      "[0.09087162 0.09087162 0.09087162 0.09087162 0.09087162 0.09087162\n",
      " 0.09087162 0.09128382 0.09087162 0.09087162 0.09087162]\n",
      "Step 41 (229) @ Episode 2/10, action 0, reward 1,loss: 2.17322516441\n",
      "[0.09087145 0.09087145 0.09087145 0.09087145 0.09087145 0.09087145\n",
      " 0.09087145 0.09128546 0.09087145 0.09087145 0.09087145]\n",
      "Step 42 (230) @ Episode 2/10, action 4, reward -1,loss: 1.97633671761\n",
      "[0.09087129 0.09087129 0.09087129 0.09087129 0.09087129 0.09087129\n",
      " 0.09087129 0.09128709 0.09087129 0.09087129 0.09087129]\n",
      "Step 43 (231) @ Episode 2/10, action 4, reward -1,loss: 1.42896711826\n",
      "[0.09087113 0.09087113 0.09087113 0.09087113 0.09087113 0.09087113\n",
      " 0.09087113 0.09128873 0.09087113 0.09087113 0.09087113]\n",
      "Step 44 (232) @ Episode 2/10, action 10, reward -3,loss: 1.41620206833\n",
      "Episode Reward: -11 Episode Length: 45\n",
      "[0.09087096 0.09087096 0.09087096 0.09087096 0.09087096 0.09087096\n",
      " 0.09087096 0.09129036 0.09087096 0.09087096 0.09087096]\n",
      "Step 0 (233) @ Episode 3/10, action 10, reward 3,loss: 1.13038945198\n",
      "Episode Reward: 3 Episode Length: 1\n",
      "[0.0908708 0.0908708 0.0908708 0.0908708 0.0908708 0.0908708 0.0908708\n",
      " 0.091292  0.0908708 0.0908708 0.0908708]\n",
      "Step 0 (234) @ Episode 4/10, action 3, reward 1,loss: 1.97574067116\n",
      "[0.09087064 0.09087064 0.09087064 0.09087064 0.09087064 0.09087064\n",
      " 0.09087064 0.09129364 0.09087064 0.09087064 0.09087064]\n",
      "Step 1 (235) @ Episode 4/10, action 4, reward -1,loss: 1.75548195839\n",
      "[0.09087047 0.09129527 0.09087047 0.09087047 0.09087047 0.09087047\n",
      " 0.09087047 0.09087047 0.09087047 0.09087047 0.09087047]\n",
      "Step 2 (236) @ Episode 4/10, action 0, reward -1,loss: 1.67512226105\n",
      "[0.09087031 0.09129691 0.09087031 0.09087031 0.09087031 0.09087031\n",
      " 0.09087031 0.09087031 0.09087031 0.09087031 0.09087031]\n",
      "Step 3 (237) @ Episode 4/10, action 1, reward 1,loss: 1.39316499233\n",
      "[0.09087015 0.09087015 0.09087015 0.09087015 0.09087015 0.09087015\n",
      " 0.09087015 0.09129855 0.09087015 0.09087015 0.09087015]\n",
      "Step 4 (238) @ Episode 4/10, action 6, reward -1,loss: 1.42370700836\n",
      "[0.09086998 0.09130018 0.09086998 0.09086998 0.09086998 0.09086998\n",
      " 0.09086998 0.09086998 0.09086998 0.09086998 0.09086998]\n",
      "Step 5 (239) @ Episode 4/10, action 5, reward -1,loss: 1.6972322464\n",
      "[0.09086982 0.09086982 0.09086982 0.09086982 0.09086982 0.09086982\n",
      " 0.09086982 0.09130182 0.09086982 0.09086982 0.09086982]\n",
      "Step 6 (240) @ Episode 4/10, action 4, reward -1,loss: 1.23214709759\n",
      "[0.09086965 0.09130346 0.09086965 0.09086965 0.09086965 0.09086965\n",
      " 0.09086965 0.09086965 0.09086965 0.09086965 0.09086965]\n",
      "Step 7 (241) @ Episode 4/10, action 6, reward -1,loss: 2.94679450989\n",
      "[0.09086949 0.09130509 0.09086949 0.09086949 0.09086949 0.09086949\n",
      " 0.09086949 0.09086949 0.09086949 0.09086949 0.09086949]\n",
      "Step 8 (242) @ Episode 4/10, action 2, reward 1,loss: 1.46355891228\n",
      "[0.09086933 0.09130673 0.09086933 0.09086933 0.09086933 0.09086933\n",
      " 0.09086933 0.09086933 0.09086933 0.09086933 0.09086933]\n",
      "Step 9 (243) @ Episode 4/10, action 4, reward -1,loss: 1.94478332996\n",
      "[0.09086916 0.09130836 0.09086916 0.09086916 0.09086916 0.09086916\n",
      " 0.09086916 0.09086916 0.09086916 0.09086916 0.09086916]\n",
      "Step 10 (244) @ Episode 4/10, action 2, reward 1,loss: 1.6856777668\n",
      "[0.090869 0.09131  0.090869 0.090869 0.090869 0.090869 0.090869 0.090869\n",
      " 0.090869 0.090869 0.090869]\n",
      "Step 11 (245) @ Episode 4/10, action 2, reward -1,loss: 1.70023369789\n",
      "[0.09086884 0.09131164 0.09086884 0.09086884 0.09086884 0.09086884\n",
      " 0.09086884 0.09086884 0.09086884 0.09086884 0.09086884]\n",
      "Step 12 (246) @ Episode 4/10, action 6, reward -1,loss: 1.51636862755\n",
      "[0.09086867 0.09131327 0.09086867 0.09086867 0.09086867 0.09086867\n",
      " 0.09086867 0.09086867 0.09086867 0.09086867 0.09086867]\n",
      "Step 13 (247) @ Episode 4/10, action 9, reward -1,loss: 1.7403178215\n",
      "[0.09086851 0.09131491 0.09086851 0.09086851 0.09086851 0.09086851\n",
      " 0.09086851 0.09086851 0.09086851 0.09086851 0.09086851]\n",
      "Step 14 (248) @ Episode 4/10, action 5, reward -1,loss: 1.67241978645\n",
      "[0.09086835 0.09131655 0.09086835 0.09086835 0.09086835 0.09086835\n",
      " 0.09086835 0.09086835 0.09086835 0.09086835 0.09086835]\n",
      "Step 15 (249) @ Episode 4/10, action 3, reward 1,loss: 1.15645837784\n",
      "[0.09086818 0.09131818 0.09086818 0.09086818 0.09086818 0.09086818\n",
      " 0.09086818 0.09086818 0.09086818 0.09086818 0.09086818]\n",
      "Step 16 (250) @ Episode 4/10, action 5, reward -1,loss: 2.12224054337\n",
      "[0.09086802 0.09086802 0.09086802 0.09131982 0.09086802 0.09086802\n",
      " 0.09086802 0.09086802 0.09086802 0.09086802 0.09086802]\n",
      "Step 17 (251) @ Episode 4/10, action 2, reward 1,loss: 0.949937462807\n",
      "[0.09086785 0.09132146 0.09086785 0.09086785 0.09086785 0.09086785\n",
      " 0.09086785 0.09086785 0.09086785 0.09086785 0.09086785]\n",
      "Step 18 (252) @ Episode 4/10, action 5, reward -1,loss: 1.34258913994\n",
      "[0.09086769 0.09132309 0.09086769 0.09086769 0.09086769 0.09086769\n",
      " 0.09086769 0.09086769 0.09086769 0.09086769 0.09086769]\n",
      "Step 19 (253) @ Episode 4/10, action 0, reward 1,loss: 2.3332824707\n",
      "[0.09086753 0.09132473 0.09086753 0.09086753 0.09086753 0.09086753\n",
      " 0.09086753 0.09086753 0.09086753 0.09086753 0.09086753]\n",
      "Step 20 (254) @ Episode 4/10, action 5, reward -1,loss: 1.68296265602\n",
      "[0.09086736 0.09132636 0.09086736 0.09086736 0.09086736 0.09086736\n",
      " 0.09086736 0.09086736 0.09086736 0.09086736 0.09086736]\n",
      "Step 21 (255) @ Episode 4/10, action 5, reward -1,loss: 1.47077727318\n",
      "[0.0908672 0.091328  0.0908672 0.0908672 0.0908672 0.0908672 0.0908672\n",
      " 0.0908672 0.0908672 0.0908672 0.0908672]\n",
      "Step 22 (256) @ Episode 4/10, action 4, reward -1,loss: 1.88334393501\n",
      "[0.09086704 0.09132964 0.09086704 0.09086704 0.09086704 0.09086704\n",
      " 0.09086704 0.09086704 0.09086704 0.09086704 0.09086704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23 (257) @ Episode 4/10, action 3, reward 1,loss: 1.38238310814\n",
      "[0.09086687 0.09133127 0.09086687 0.09086687 0.09086687 0.09086687\n",
      " 0.09086687 0.09086687 0.09086687 0.09086687 0.09086687]\n",
      "Step 24 (258) @ Episode 4/10, action 5, reward -1,loss: 1.6644487381\n",
      "[0.09086671 0.09133291 0.09086671 0.09086671 0.09086671 0.09086671\n",
      " 0.09086671 0.09086671 0.09086671 0.09086671 0.09086671]\n",
      "Step 25 (259) @ Episode 4/10, action 3, reward 1,loss: 2.68202257156\n",
      "[0.09086655 0.09133455 0.09086655 0.09086655 0.09086655 0.09086655\n",
      " 0.09086655 0.09086655 0.09086655 0.09086655 0.09086655]\n",
      "Step 26 (260) @ Episode 4/10, action 5, reward -1,loss: 1.70678186417\n",
      "[0.09086638 0.09133618 0.09086638 0.09086638 0.09086638 0.09086638\n",
      " 0.09086638 0.09086638 0.09086638 0.09086638 0.09086638]\n",
      "Step 27 (261) @ Episode 4/10, action 7, reward -1,loss: 1.4281693697\n",
      "[0.09086622 0.09133782 0.09086622 0.09086622 0.09086622 0.09086622\n",
      " 0.09086622 0.09086622 0.09086622 0.09086622 0.09086622]\n",
      "Step 28 (262) @ Episode 4/10, action 6, reward -1,loss: 2.45187401772\n",
      "[0.09086605 0.09133946 0.09086605 0.09086605 0.09086605 0.09086605\n",
      " 0.09086605 0.09086605 0.09086605 0.09086605 0.09086605]\n",
      "Step 29 (263) @ Episode 4/10, action 2, reward 1,loss: 2.36508369446\n",
      "[0.09086589 0.09134109 0.09086589 0.09086589 0.09086589 0.09086589\n",
      " 0.09086589 0.09086589 0.09086589 0.09086589 0.09086589]\n",
      "Step 30 (264) @ Episode 4/10, action 9, reward -1,loss: 1.19128072262\n",
      "[0.09086573 0.09134273 0.09086573 0.09086573 0.09086573 0.09086573\n",
      " 0.09086573 0.09086573 0.09086573 0.09086573 0.09086573]\n",
      "Step 31 (265) @ Episode 4/10, action 7, reward -1,loss: 1.34344482422\n",
      "[0.09086556 0.09134436 0.09086556 0.09086556 0.09086556 0.09086556\n",
      " 0.09086556 0.09086556 0.09086556 0.09086556 0.09086556]\n",
      "Step 32 (266) @ Episode 4/10, action 4, reward -1,loss: 2.22029733658\n",
      "[0.0908654 0.0908654 0.0908654 0.0908654 0.0908654 0.0908654 0.0908654\n",
      " 0.091346  0.0908654 0.0908654 0.0908654]\n",
      "Step 33 (267) @ Episode 4/10, action 9, reward -1,loss: 1.42740106583\n",
      "[0.09086524 0.09134764 0.09086524 0.09086524 0.09086524 0.09086524\n",
      " 0.09086524 0.09086524 0.09086524 0.09086524 0.09086524]\n",
      "Step 34 (268) @ Episode 4/10, action 4, reward -1,loss: 1.92531526089\n",
      "[0.09086507 0.09086507 0.09086507 0.09086507 0.09086507 0.09086507\n",
      " 0.09086507 0.09134927 0.09086507 0.09086507 0.09086507]\n",
      "Step 35 (269) @ Episode 4/10, action 8, reward -1,loss: 1.44299888611\n",
      "[0.09086491 0.09086491 0.09086491 0.09086491 0.09086491 0.09086491\n",
      " 0.09086491 0.09135091 0.09086491 0.09086491 0.09086491]\n",
      "Step 36 (270) @ Episode 4/10, action 3, reward 1,loss: 1.39319932461\n",
      "[0.09086475 0.09086475 0.09086475 0.09086475 0.09086475 0.09086475\n",
      " 0.09086475 0.09135255 0.09086475 0.09086475 0.09086475]\n",
      "Step 37 (271) @ Episode 4/10, action 3, reward 1,loss: 1.8031938076\n",
      "[0.09086458 0.09086458 0.09086458 0.09086458 0.09086458 0.09086458\n",
      " 0.09086458 0.09135418 0.09086458 0.09086458 0.09086458]\n",
      "Step 38 (272) @ Episode 4/10, action 1, reward 1,loss: 1.88645422459\n",
      "[0.09086442 0.09086442 0.09086442 0.09086442 0.09086442 0.09086442\n",
      " 0.09086442 0.09135582 0.09086442 0.09086442 0.09086442]\n",
      "Step 39 (273) @ Episode 4/10, action 9, reward -1,loss: 2.43558263779\n",
      "[0.09086425 0.09086425 0.09086425 0.09086425 0.09086425 0.09086425\n",
      " 0.09086425 0.09135746 0.09086425 0.09086425 0.09086425]\n",
      "Step 40 (274) @ Episode 4/10, action 10, reward -3,loss: 1.65981841087\n",
      "Episode Reward: -17 Episode Length: 41\n",
      "[0.09086409 0.09135909 0.09086409 0.09086409 0.09086409 0.09086409\n",
      " 0.09086409 0.09086409 0.09086409 0.09086409 0.09086409]\n",
      "Step 0 (275) @ Episode 5/10, action 5, reward 1,loss: 1.8245152235\n",
      "[0.09086393 0.09136073 0.09086393 0.09086393 0.09086393 0.09086393\n",
      " 0.09086393 0.09086393 0.09086393 0.09086393 0.09086393]\n",
      "Step 1 (276) @ Episode 5/10, action 1, reward 1,loss: 1.9061871767\n",
      "[0.09086376 0.09086376 0.09086376 0.09086376 0.09086376 0.09086376\n",
      " 0.09086376 0.09136236 0.09086376 0.09086376 0.09086376]\n",
      "Step 2 (277) @ Episode 5/10, action 2, reward -1,loss: 1.40993022919\n",
      "[0.0908636 0.091364  0.0908636 0.0908636 0.0908636 0.0908636 0.0908636\n",
      " 0.0908636 0.0908636 0.0908636 0.0908636]\n",
      "Step 3 (278) @ Episode 5/10, action 10, reward 3,loss: 2.26447868347\n",
      "Episode Reward: 4 Episode Length: 4\n",
      "[0.09086344 0.09136564 0.09086344 0.09086344 0.09086344 0.09086344\n",
      " 0.09086344 0.09086344 0.09086344 0.09086344 0.09086344]\n",
      "Step 0 (279) @ Episode 6/10, action 8, reward 1,loss: 1.36086821556\n",
      "[0.09086327 0.09136727 0.09086327 0.09086327 0.09086327 0.09086327\n",
      " 0.09086327 0.09086327 0.09086327 0.09086327 0.09086327]\n",
      "Step 1 (280) @ Episode 6/10, action 2, reward -1,loss: 1.38963270187\n",
      "[0.09086311 0.09136891 0.09086311 0.09086311 0.09086311 0.09086311\n",
      " 0.09086311 0.09086311 0.09086311 0.09086311 0.09086311]\n",
      "Step 2 (281) @ Episode 6/10, action 8, reward -1,loss: 1.34142446518\n",
      "[0.09086295 0.09086295 0.09086295 0.09086295 0.09086295 0.09086295\n",
      " 0.09086295 0.09137055 0.09086295 0.09086295 0.09086295]\n",
      "Step 3 (282) @ Episode 6/10, action 3, reward -1,loss: 1.34450101852\n",
      "[0.09086278 0.09086278 0.09086278 0.09086278 0.09086278 0.09086278\n",
      " 0.09086278 0.09137218 0.09086278 0.09086278 0.09086278]\n",
      "Step 4 (283) @ Episode 6/10, action 1, reward -1,loss: 2.11988687515\n",
      "[0.09086262 0.09086262 0.09086262 0.09086262 0.09086262 0.09086262\n",
      " 0.09086262 0.09137382 0.09086262 0.09086262 0.09086262]\n",
      "Step 5 (284) @ Episode 6/10, action 9, reward -1,loss: 1.1640329361\n",
      "[0.09086245 0.09137546 0.09086245 0.09086245 0.09086245 0.09086245\n",
      " 0.09086245 0.09086245 0.09086245 0.09086245 0.09086245]\n",
      "Step 6 (285) @ Episode 6/10, action 8, reward -1,loss: 2.14489197731\n",
      "[0.09086229 0.09086229 0.09086229 0.09086229 0.09086229 0.09086229\n",
      " 0.09086229 0.09137709 0.09086229 0.09086229 0.09086229]\n",
      "Step 7 (286) @ Episode 6/10, action 2, reward 1,loss: 1.46506881714\n",
      "[0.09086213 0.09086213 0.09086213 0.09086213 0.09086213 0.09086213\n",
      " 0.09086213 0.09137873 0.09086213 0.09086213 0.09086213]\n",
      "Step 8 (287) @ Episode 6/10, action 2, reward 1,loss: 2.46959018707\n",
      "[0.09086196 0.09086196 0.09086196 0.09086196 0.09086196 0.09086196\n",
      " 0.09086196 0.09138036 0.09086196 0.09086196 0.09086196]\n",
      "Step 9 (288) @ Episode 6/10, action 5, reward -1,loss: 1.40226340294\n",
      "[0.0908618 0.0908618 0.0908618 0.0908618 0.0908618 0.0908618 0.0908618\n",
      " 0.091382  0.0908618 0.0908618 0.0908618]\n",
      "Step 10 (289) @ Episode 6/10, action 5, reward -1,loss: 1.58310604095\n",
      "[0.09086164 0.09086164 0.09086164 0.09086164 0.09086164 0.09086164\n",
      " 0.09086164 0.09138364 0.09086164 0.09086164 0.09086164]\n",
      "Step 11 (290) @ Episode 6/10, action 1, reward 1,loss: 1.61376523972\n",
      "[0.09086147 0.09086147 0.09086147 0.09086147 0.09086147 0.09086147\n",
      " 0.09086147 0.09138527 0.09086147 0.09086147 0.09086147]\n",
      "Step 12 (291) @ Episode 6/10, action 7, reward 1,loss: 1.18369460106\n",
      "[0.09086131 0.09086131 0.09086131 0.09086131 0.09086131 0.09086131\n",
      " 0.09086131 0.09138691 0.09086131 0.09086131 0.09086131]\n",
      "Step 13 (292) @ Episode 6/10, action 0, reward 1,loss: 0.985742330551\n",
      "[0.09086115 0.09086115 0.09086115 0.09086115 0.09086115 0.09086115\n",
      " 0.09086115 0.09138855 0.09086115 0.09086115 0.09086115]\n",
      "Step 14 (293) @ Episode 6/10, action 5, reward -1,loss: 1.38854122162\n",
      "[0.09086098 0.09086098 0.09086098 0.09086098 0.09086098 0.09086098\n",
      " 0.09086098 0.09139018 0.09086098 0.09086098 0.09086098]\n",
      "Step 15 (294) @ Episode 6/10, action 2, reward 1,loss: 1.59547042847\n",
      "[0.09086082 0.09086082 0.09086082 0.09086082 0.09086082 0.09086082\n",
      " 0.09086082 0.09139182 0.09086082 0.09086082 0.09086082]\n",
      "Step 16 (295) @ Episode 6/10, action 8, reward -1,loss: 2.06353855133\n",
      "[0.09086065 0.09086065 0.09086065 0.09086065 0.09086065 0.09086065\n",
      " 0.09086065 0.09139346 0.09086065 0.09086065 0.09086065]\n",
      "Step 17 (296) @ Episode 6/10, action 4, reward -1,loss: 1.65912723541\n",
      "[0.09086049 0.09086049 0.09086049 0.09086049 0.09086049 0.09086049\n",
      " 0.09086049 0.09139509 0.09086049 0.09086049 0.09086049]\n",
      "Step 18 (297) @ Episode 6/10, action 10, reward -3,loss: 1.42872834206\n",
      "Episode Reward: -7 Episode Length: 19\n",
      "[0.09086033 0.09139673 0.09086033 0.09086033 0.09086033 0.09086033\n",
      " 0.09086033 0.09086033 0.09086033 0.09086033 0.09086033]\n",
      "Step 0 (298) @ Episode 7/10, action 4, reward 1,loss: 1.35306501389\n",
      "[0.09086016 0.09139836 0.09086016 0.09086016 0.09086016 0.09086016\n",
      " 0.09086016 0.09086016 0.09086016 0.09086016 0.09086016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (299) @ Episode 7/10, action 10, reward 3,loss: 1.90715265274\n",
      "Episode Reward: 4 Episode Length: 2\n",
      "[0.09086 0.0914  0.09086 0.09086 0.09086 0.09086 0.09086 0.09086 0.09086\n",
      " 0.09086 0.09086]\n",
      "Step 0 (300) @ Episode 8/10, action 0, reward 1,loss: 1.89640617371\n",
      "[0.09085984 0.09140164 0.09085984 0.09085984 0.09085984 0.09085984\n",
      " 0.09085984 0.09085984 0.09085984 0.09085984 0.09085984]\n",
      "Step 1 (301) @ Episode 8/10, action 1, reward 1,loss: 1.40194499493\n",
      "[0.09085967 0.09140327 0.09085967 0.09085967 0.09085967 0.09085967\n",
      " 0.09085967 0.09085967 0.09085967 0.09085967 0.09085967]\n",
      "Step 2 (302) @ Episode 8/10, action 6, reward -1,loss: 0.852534413338\n",
      "[0.09085951 0.09140491 0.09085951 0.09085951 0.09085951 0.09085951\n",
      " 0.09085951 0.09085951 0.09085951 0.09085951 0.09085951]\n",
      "Step 3 (303) @ Episode 8/10, action 3, reward 1,loss: 1.39106142521\n",
      "[0.09085935 0.09085935 0.09085935 0.09085935 0.09085935 0.09085935\n",
      " 0.09085935 0.09140655 0.09085935 0.09085935 0.09085935]\n",
      "Step 4 (304) @ Episode 8/10, action 2, reward -1,loss: 1.98793053627\n",
      "[0.09085918 0.09140818 0.09085918 0.09085918 0.09085918 0.09085918\n",
      " 0.09085918 0.09085918 0.09085918 0.09085918 0.09085918]\n",
      "Step 5 (305) @ Episode 8/10, action 9, reward -1,loss: 1.97030711174\n",
      "[0.09085902 0.09140982 0.09085902 0.09085902 0.09085902 0.09085902\n",
      " 0.09085902 0.09085902 0.09085902 0.09085902 0.09085902]\n",
      "Step 6 (306) @ Episode 8/10, action 5, reward -1,loss: 1.91174674034\n",
      "[0.09085885 0.09141146 0.09085885 0.09085885 0.09085885 0.09085885\n",
      " 0.09085885 0.09085885 0.09085885 0.09085885 0.09085885]\n",
      "Step 7 (307) @ Episode 8/10, action 10, reward -3,loss: 1.62136363983\n",
      "Episode Reward: -4 Episode Length: 8\n",
      "[0.09085869 0.09141309 0.09085869 0.09085869 0.09085869 0.09085869\n",
      " 0.09085869 0.09085869 0.09085869 0.09085869 0.09085869]\n",
      "Step 0 (308) @ Episode 9/10, action 9, reward 1,loss: 1.42346596718\n",
      "[0.09085853 0.09141473 0.09085853 0.09085853 0.09085853 0.09085853\n",
      " 0.09085853 0.09085853 0.09085853 0.09085853 0.09085853]\n",
      "Step 1 (309) @ Episode 9/10, action 2, reward -1,loss: 1.39579510689\n",
      "[0.09085836 0.09141636 0.09085836 0.09085836 0.09085836 0.09085836\n",
      " 0.09085836 0.09085836 0.09085836 0.09085836 0.09085836]\n",
      "Step 2 (310) @ Episode 9/10, action 3, reward 1,loss: 1.84450745583\n",
      "[0.0908582 0.0908582 0.0908582 0.0908582 0.0908582 0.0908582 0.0908582\n",
      " 0.091418  0.0908582 0.0908582 0.0908582]\n",
      "Step 3 (311) @ Episode 9/10, action 4, reward -1,loss: 1.64925539494\n",
      "[0.09085804 0.09141964 0.09085804 0.09085804 0.09085804 0.09085804\n",
      " 0.09085804 0.09085804 0.09085804 0.09085804 0.09085804]\n",
      "Step 4 (312) @ Episode 9/10, action 4, reward -1,loss: 2.252866745\n",
      "[0.09085787 0.09142127 0.09085787 0.09085787 0.09085787 0.09085787\n",
      " 0.09085787 0.09085787 0.09085787 0.09085787 0.09085787]\n",
      "Step 5 (313) @ Episode 9/10, action 1, reward 1,loss: 1.40050768852\n",
      "[0.09085771 0.09142291 0.09085771 0.09085771 0.09085771 0.09085771\n",
      " 0.09085771 0.09085771 0.09085771 0.09085771 0.09085771]\n",
      "Step 6 (314) @ Episode 9/10, action 9, reward -1,loss: 1.76268661022\n",
      "[0.09085755 0.09142455 0.09085755 0.09085755 0.09085755 0.09085755\n",
      " 0.09085755 0.09085755 0.09085755 0.09085755 0.09085755]\n",
      "Step 7 (315) @ Episode 9/10, action 1, reward -1,loss: 1.19690012932\n",
      "[0.09085738 0.09142618 0.09085738 0.09085738 0.09085738 0.09085738\n",
      " 0.09085738 0.09085738 0.09085738 0.09085738 0.09085738]\n",
      "Step 8 (316) @ Episode 9/10, action 2, reward -1,loss: 1.64467573166\n",
      "[0.09085722 0.09142782 0.09085722 0.09085722 0.09085722 0.09085722\n",
      " 0.09085722 0.09085722 0.09085722 0.09085722 0.09085722]\n",
      "Step 9 (317) @ Episode 9/10, action 1, reward -1,loss: 2.00149989128\n",
      "[0.09085705 0.09142946 0.09085705 0.09085705 0.09085705 0.09085705\n",
      " 0.09085705 0.09085705 0.09085705 0.09085705 0.09085705]\n",
      "Step 10 (318) @ Episode 9/10, action 10, reward -3,loss: 1.55695450306\n",
      "Episode Reward: -7 Episode Length: 11\n",
      "[0.09085689 0.09143109 0.09085689 0.09085689 0.09085689 0.09085689\n",
      " 0.09085689 0.09085689 0.09085689 0.09085689 0.09085689]\n",
      "Step 0 (319) @ Episode 10/10, action 4, reward 1,loss: 1.43175911903\n",
      "[0.09085673 0.09143273 0.09085673 0.09085673 0.09085673 0.09085673\n",
      " 0.09085673 0.09085673 0.09085673 0.09085673 0.09085673]\n",
      "Step 1 (320) @ Episode 10/10, action 9, reward -1,loss: 1.63997840881\n",
      "[0.09085656 0.09143436 0.09085656 0.09085656 0.09085656 0.09085656\n",
      " 0.09085656 0.09085656 0.09085656 0.09085656 0.09085656]\n",
      "Step 2 (321) @ Episode 10/10, action 8, reward -1,loss: 2.42874741554\n",
      "[0.0908564 0.091436  0.0908564 0.0908564 0.0908564 0.0908564 0.0908564\n",
      " 0.0908564 0.0908564 0.0908564 0.0908564]\n",
      "Step 3 (322) @ Episode 10/10, action 3, reward 1,loss: 1.40690112114\n",
      "[0.09085624 0.09143764 0.09085624 0.09085624 0.09085624 0.09085624\n",
      " 0.09085624 0.09085624 0.09085624 0.09085624 0.09085624]\n",
      "Step 4 (323) @ Episode 10/10, action 6, reward -1,loss: 1.31455123425\n",
      "[0.09085607 0.09143927 0.09085607 0.09085607 0.09085607 0.09085607\n",
      " 0.09085607 0.09085607 0.09085607 0.09085607 0.09085607]\n",
      "Step 5 (324) @ Episode 10/10, action 3, reward 1,loss: 2.59031224251\n",
      "[0.09085591 0.09085591 0.09085591 0.09085591 0.09085591 0.09085591\n",
      " 0.09085591 0.09144091 0.09085591 0.09085591 0.09085591]\n",
      "Step 6 (325) @ Episode 10/10, action 1, reward 1,loss: 1.91207957268\n",
      "[0.09085575 0.09144255 0.09085575 0.09085575 0.09085575 0.09085575\n",
      " 0.09085575 0.09085575 0.09085575 0.09085575 0.09085575]\n",
      "Step 7 (326) @ Episode 10/10, action 10, reward -3,loss: 1.68093383312\n",
      "Episode Reward: -2 Episode Length: 8\n",
      "New image is being loaded: 2008_007702.jpg\n",
      "[0.09085558 0.09085558 0.09085558 0.09085558 0.09085558 0.09085558\n",
      " 0.09085558 0.09144418 0.09085558 0.09085558 0.09085558]\n",
      "Step 0 (327) @ Episode 1/10, action 0, reward 1,loss: 1.88318252563\n",
      "[0.09085542 0.09085542 0.09085542 0.09085542 0.09085542 0.09085542\n",
      " 0.09085542 0.09144582 0.09085542 0.09085542 0.09085542]\n",
      "Step 1 (328) @ Episode 1/10, action 6, reward 1,loss: 2.36211299896\n",
      "[0.09085525 0.09085525 0.09085525 0.09085525 0.09085525 0.09085525\n",
      " 0.09085525 0.09144746 0.09085525 0.09085525 0.09085525]\n",
      "Step 2 (329) @ Episode 1/10, action 8, reward -1,loss: 1.10550415516\n",
      "[0.09085509 0.09144909 0.09085509 0.09085509 0.09085509 0.09085509\n",
      " 0.09085509 0.09085509 0.09085509 0.09085509 0.09085509]\n",
      "Step 3 (330) @ Episode 1/10, action 9, reward -1,loss: 1.04540610313\n",
      "[0.09085493 0.09145073 0.09085493 0.09085493 0.09085493 0.09085493\n",
      " 0.09085493 0.09085493 0.09085493 0.09085493 0.09085493]\n",
      "Step 4 (331) @ Episode 1/10, action 7, reward 1,loss: 1.48125934601\n",
      "[0.09085476 0.09145236 0.09085476 0.09085476 0.09085476 0.09085476\n",
      " 0.09085476 0.09085476 0.09085476 0.09085476 0.09085476]\n",
      "Step 5 (332) @ Episode 1/10, action 5, reward -1,loss: 1.0559322834\n",
      "[0.0908546 0.091454  0.0908546 0.0908546 0.0908546 0.0908546 0.0908546\n",
      " 0.0908546 0.0908546 0.0908546 0.0908546]\n",
      "Step 6 (333) @ Episode 1/10, action 4, reward -1,loss: 1.63474059105\n",
      "[0.09085444 0.09145564 0.09085444 0.09085444 0.09085444 0.09085444\n",
      " 0.09085444 0.09085444 0.09085444 0.09085444 0.09085444]\n",
      "Step 7 (334) @ Episode 1/10, action 10, reward -3,loss: 1.21688783169\n",
      "Episode Reward: -4 Episode Length: 8\n",
      "[0.09085427 0.09145727 0.09085427 0.09085427 0.09085427 0.09085427\n",
      " 0.09085427 0.09085427 0.09085427 0.09085427 0.09085427]\n",
      "Step 0 (335) @ Episode 2/10, action 4, reward 1,loss: 1.84841251373\n",
      "[0.09085411 0.09145891 0.09085411 0.09085411 0.09085411 0.09085411\n",
      " 0.09085411 0.09085411 0.09085411 0.09085411 0.09085411]\n",
      "Step 1 (336) @ Episode 2/10, action 10, reward -3,loss: 1.86704111099\n",
      "Episode Reward: -2 Episode Length: 2\n",
      "[0.09085395 0.09146055 0.09085395 0.09085395 0.09085395 0.09085395\n",
      " 0.09085395 0.09085395 0.09085395 0.09085395 0.09085395]\n",
      "Step 0 (337) @ Episode 3/10, action 5, reward 1,loss: 2.88337802887\n",
      "[0.09085378 0.09146218 0.09085378 0.09085378 0.09085378 0.09085378\n",
      " 0.09085378 0.09085378 0.09085378 0.09085378 0.09085378]\n",
      "Step 1 (338) @ Episode 3/10, action 5, reward -1,loss: 2.59852600098\n",
      "[0.09085362 0.09146382 0.09085362 0.09085362 0.09085362 0.09085362\n",
      " 0.09085362 0.09085362 0.09085362 0.09085362 0.09085362]\n",
      "Step 2 (339) @ Episode 3/10, action 1, reward 1,loss: 0.920398950577\n",
      "[0.09085345 0.09146546 0.09085345 0.09085345 0.09085345 0.09085345\n",
      " 0.09085345 0.09085345 0.09085345 0.09085345 0.09085345]\n",
      "Step 3 (340) @ Episode 3/10, action 10, reward -3,loss: 1.96923661232\n",
      "Episode Reward: -2 Episode Length: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09085329 0.09146709 0.09085329 0.09085329 0.09085329 0.09085329\n",
      " 0.09085329 0.09085329 0.09085329 0.09085329 0.09085329]\n",
      "Step 0 (341) @ Episode 4/10, action 0, reward 1,loss: 1.40719175339\n",
      "[0.09085313 0.09146873 0.09085313 0.09085313 0.09085313 0.09085313\n",
      " 0.09085313 0.09085313 0.09085313 0.09085313 0.09085313]\n",
      "Step 1 (342) @ Episode 4/10, action 1, reward 1,loss: 2.10588383675\n",
      "[0.09085296 0.09147036 0.09085296 0.09085296 0.09085296 0.09085296\n",
      " 0.09085296 0.09085296 0.09085296 0.09085296 0.09085296]\n",
      "Step 2 (343) @ Episode 4/10, action 10, reward -3,loss: 1.3426579237\n",
      "Episode Reward: -1 Episode Length: 3\n",
      "[0.0908528 0.091472  0.0908528 0.0908528 0.0908528 0.0908528 0.0908528\n",
      " 0.0908528 0.0908528 0.0908528 0.0908528]\n",
      "Step 0 (344) @ Episode 5/10, action 3, reward 1,loss: 1.13160634041\n",
      "[0.09085264 0.09147364 0.09085264 0.09085264 0.09085264 0.09085264\n",
      " 0.09085264 0.09085264 0.09085264 0.09085264 0.09085264]\n",
      "Step 1 (345) @ Episode 5/10, action 8, reward 1,loss: 1.13038051128\n",
      "[0.09085247 0.09085247 0.09085247 0.09085247 0.09085247 0.09085247\n",
      " 0.09085247 0.09147527 0.09085247 0.09085247 0.09085247]\n",
      "Step 2 (346) @ Episode 5/10, action 6, reward -1,loss: 1.73437392712\n",
      "[0.09085231 0.09147691 0.09085231 0.09085231 0.09085231 0.09085231\n",
      " 0.09085231 0.09085231 0.09085231 0.09085231 0.09085231]\n",
      "Step 3 (347) @ Episode 5/10, action 3, reward -1,loss: 2.20863723755\n",
      "[0.09085215 0.09147855 0.09085215 0.09085215 0.09085215 0.09085215\n",
      " 0.09085215 0.09085215 0.09085215 0.09085215 0.09085215]\n",
      "Step 4 (348) @ Episode 5/10, action 0, reward -1,loss: 1.19212198257\n",
      "[0.09085198 0.09148018 0.09085198 0.09085198 0.09085198 0.09085198\n",
      " 0.09085198 0.09085198 0.09085198 0.09085198 0.09085198]\n",
      "Step 5 (349) @ Episode 5/10, action 1, reward -1,loss: 1.6585060358\n",
      "[0.09085182 0.09148182 0.09085182 0.09085182 0.09085182 0.09085182\n",
      " 0.09085182 0.09085182 0.09085182 0.09085182 0.09085182]\n",
      "Step 6 (350) @ Episode 5/10, action 4, reward -1,loss: 1.90762197971\n",
      "[0.09085165 0.09148346 0.09085165 0.09085165 0.09085165 0.09085165\n",
      " 0.09085165 0.09085165 0.09085165 0.09085165 0.09085165]\n",
      "Step 7 (351) @ Episode 5/10, action 10, reward -3,loss: 1.39374279976\n",
      "Episode Reward: -6 Episode Length: 8\n",
      "[0.09085149 0.09148509 0.09085149 0.09085149 0.09085149 0.09085149\n",
      " 0.09085149 0.09085149 0.09085149 0.09085149 0.09085149]\n",
      "Step 0 (352) @ Episode 6/10, action 9, reward 1,loss: 1.66087663174\n",
      "[0.09085133 0.09085133 0.09085133 0.09085133 0.09085133 0.09085133\n",
      " 0.09085133 0.09148673 0.09085133 0.09085133 0.09085133]\n",
      "Step 1 (353) @ Episode 6/10, action 9, reward -1,loss: 1.81533765793\n",
      "[0.09085116 0.09085116 0.09085116 0.09085116 0.09085116 0.09085116\n",
      " 0.09085116 0.09148836 0.09085116 0.09085116 0.09085116]\n",
      "Step 2 (354) @ Episode 6/10, action 9, reward -1,loss: 1.7527756691\n",
      "[0.090851 0.09149  0.090851 0.090851 0.090851 0.090851 0.090851 0.090851\n",
      " 0.090851 0.090851 0.090851]\n",
      "Step 3 (355) @ Episode 6/10, action 10, reward -3,loss: 1.40979290009\n",
      "Episode Reward: -4 Episode Length: 4\n",
      "[0.09085084 0.09149164 0.09085084 0.09085084 0.09085084 0.09085084\n",
      " 0.09085084 0.09085084 0.09085084 0.09085084 0.09085084]\n",
      "Step 0 (356) @ Episode 7/10, action 6, reward 1,loss: 1.37745726109\n",
      "[0.09085067 0.09085067 0.09085067 0.09085067 0.09085067 0.09085067\n",
      " 0.09085067 0.09149327 0.09085067 0.09085067 0.09085067]\n",
      "Step 1 (357) @ Episode 7/10, action 1, reward -1,loss: 0.854288816452\n",
      "[0.09085051 0.09085051 0.09085051 0.09085051 0.09085051 0.09085051\n",
      " 0.09085051 0.09149491 0.09085051 0.09085051 0.09085051]\n",
      "Step 2 (358) @ Episode 7/10, action 4, reward 1,loss: 2.2464659214\n",
      "[0.09085035 0.09085035 0.09085035 0.09085035 0.09085035 0.09085035\n",
      " 0.09085035 0.09149655 0.09085035 0.09085035 0.09085035]\n",
      "Step 3 (359) @ Episode 7/10, action 0, reward -1,loss: 1.43279981613\n",
      "[0.09085018 0.09085018 0.09085018 0.09085018 0.09085018 0.09085018\n",
      " 0.09085018 0.09149818 0.09085018 0.09085018 0.09085018]\n",
      "Step 4 (360) @ Episode 7/10, action 3, reward -1,loss: 1.52162468433\n",
      "[0.09085002 0.09085002 0.09085002 0.09085002 0.09085002 0.09085002\n",
      " 0.09085002 0.09149982 0.09085002 0.09085002 0.09085002]\n",
      "Step 5 (361) @ Episode 7/10, action 9, reward -1,loss: 1.31321692467\n",
      "[0.09084985 0.09084985 0.09084985 0.09084985 0.09084985 0.09084985\n",
      " 0.09084985 0.09150146 0.09084985 0.09084985 0.09084985]\n",
      "Step 6 (362) @ Episode 7/10, action 4, reward 1,loss: 1.38549494743\n",
      "[0.09084969 0.09084969 0.09084969 0.09084969 0.09084969 0.09084969\n",
      " 0.09084969 0.09150309 0.09084969 0.09084969 0.09084969]\n",
      "Step 7 (363) @ Episode 7/10, action 6, reward -1,loss: 1.53525328636\n",
      "[0.09084953 0.09084953 0.09084953 0.09084953 0.09084953 0.09084953\n",
      " 0.09084953 0.09150473 0.09084953 0.09084953 0.09084953]\n",
      "Step 8 (364) @ Episode 7/10, action 4, reward 1,loss: 1.76404881477\n",
      "[0.09084936 0.09150636 0.09084936 0.09084936 0.09084936 0.09084936\n",
      " 0.09084936 0.09084936 0.09084936 0.09084936 0.09084936]\n",
      "Step 9 (365) @ Episode 7/10, action 2, reward -1,loss: 1.90724718571\n",
      "[0.0908492 0.091508  0.0908492 0.0908492 0.0908492 0.0908492 0.0908492\n",
      " 0.0908492 0.0908492 0.0908492 0.0908492]\n",
      "Step 10 (366) @ Episode 7/10, action 8, reward -1,loss: 1.59010744095\n",
      "[0.09084904 0.09150964 0.09084904 0.09084904 0.09084904 0.09084904\n",
      " 0.09084904 0.09084904 0.09084904 0.09084904 0.09084904]\n",
      "Step 11 (367) @ Episode 7/10, action 8, reward -1,loss: 1.87622642517\n",
      "[0.09084887 0.09151127 0.09084887 0.09084887 0.09084887 0.09084887\n",
      " 0.09084887 0.09084887 0.09084887 0.09084887 0.09084887]\n",
      "Step 12 (368) @ Episode 7/10, action 0, reward 1,loss: 1.63395142555\n",
      "[0.09084871 0.09151291 0.09084871 0.09084871 0.09084871 0.09084871\n",
      " 0.09084871 0.09084871 0.09084871 0.09084871 0.09084871]\n",
      "Step 13 (369) @ Episode 7/10, action 5, reward -1,loss: 1.437317729\n",
      "[0.09084855 0.09151455 0.09084855 0.09084855 0.09084855 0.09084855\n",
      " 0.09084855 0.09084855 0.09084855 0.09084855 0.09084855]\n",
      "Step 14 (370) @ Episode 7/10, action 1, reward 1,loss: 1.44682002068\n",
      "[0.09084838 0.09151618 0.09084838 0.09084838 0.09084838 0.09084838\n",
      " 0.09084838 0.09084838 0.09084838 0.09084838 0.09084838]\n",
      "Step 15 (371) @ Episode 7/10, action 9, reward -1,loss: 2.13715291023\n",
      "[0.09084822 0.09151782 0.09084822 0.09084822 0.09084822 0.09084822\n",
      " 0.09084822 0.09084822 0.09084822 0.09084822 0.09084822]\n",
      "Step 16 (372) @ Episode 7/10, action 0, reward -1,loss: 1.07075858116\n",
      "[0.09084805 0.09151946 0.09084805 0.09084805 0.09084805 0.09084805\n",
      " 0.09084805 0.09084805 0.09084805 0.09084805 0.09084805]\n",
      "Step 17 (373) @ Episode 7/10, action 1, reward 1,loss: 1.88950049877\n",
      "[0.09084789 0.09152109 0.09084789 0.09084789 0.09084789 0.09084789\n",
      " 0.09084789 0.09084789 0.09084789 0.09084789 0.09084789]\n",
      "Step 18 (374) @ Episode 7/10, action 6, reward -1,loss: 1.86175346375\n",
      "[0.09084773 0.09152273 0.09084773 0.09084773 0.09084773 0.09084773\n",
      " 0.09084773 0.09084773 0.09084773 0.09084773 0.09084773]\n",
      "Step 19 (375) @ Episode 7/10, action 2, reward 1,loss: 1.40508925915\n",
      "[0.09084756 0.09152436 0.09084756 0.09084756 0.09084756 0.09084756\n",
      " 0.09084756 0.09084756 0.09084756 0.09084756 0.09084756]\n",
      "Step 20 (376) @ Episode 7/10, action 0, reward -1,loss: 1.92384159565\n",
      "[0.0908474 0.091526  0.0908474 0.0908474 0.0908474 0.0908474 0.0908474\n",
      " 0.0908474 0.0908474 0.0908474 0.0908474]\n",
      "Step 21 (377) @ Episode 7/10, action 5, reward -1,loss: 1.66990160942\n",
      "[0.09084724 0.09152764 0.09084724 0.09084724 0.09084724 0.09084724\n",
      " 0.09084724 0.09084724 0.09084724 0.09084724 0.09084724]\n",
      "Step 22 (378) @ Episode 7/10, action 4, reward -1,loss: 1.77735483646\n",
      "[0.09084707 0.09152927 0.09084707 0.09084707 0.09084707 0.09084707\n",
      " 0.09084707 0.09084707 0.09084707 0.09084707 0.09084707]\n",
      "Step 23 (379) @ Episode 7/10, action 8, reward -1,loss: 0.965942203999\n",
      "[0.09084691 0.09153091 0.09084691 0.09084691 0.09084691 0.09084691\n",
      " 0.09084691 0.09084691 0.09084691 0.09084691 0.09084691]\n",
      "Step 24 (380) @ Episode 7/10, action 0, reward -1,loss: 1.57409000397\n",
      "[0.09084675 0.09084675 0.09084675 0.09084675 0.09084675 0.09084675\n",
      " 0.09084675 0.09153255 0.09084675 0.09084675 0.09084675]\n",
      "Step 25 (381) @ Episode 7/10, action 7, reward 1,loss: 1.97426509857\n",
      "[0.09084658 0.09084658 0.09084658 0.09084658 0.09084658 0.09084658\n",
      " 0.09084658 0.09153418 0.09084658 0.09084658 0.09084658]\n",
      "Step 26 (382) @ Episode 7/10, action 1, reward 1,loss: 1.49243915081\n",
      "[0.09084642 0.09084642 0.09084642 0.09084642 0.09084642 0.09084642\n",
      " 0.09084642 0.09153582 0.09084642 0.09084642 0.09084642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27 (383) @ Episode 7/10, action 5, reward -1,loss: 0.977403759956\n",
      "[0.09084625 0.09084625 0.09084625 0.09084625 0.09084625 0.09084625\n",
      " 0.09084625 0.09153746 0.09084625 0.09084625 0.09084625]\n",
      "Step 28 (384) @ Episode 7/10, action 1, reward 1,loss: 2.07148432732\n",
      "[0.09084609 0.09153909 0.09084609 0.09084609 0.09084609 0.09084609\n",
      " 0.09084609 0.09084609 0.09084609 0.09084609 0.09084609]\n",
      "Step 29 (385) @ Episode 7/10, action 6, reward -1,loss: 1.05658447742\n",
      "[0.09084593 0.09084593 0.09084593 0.09084593 0.09084593 0.09084593\n",
      " 0.09084593 0.09154073 0.09084593 0.09084593 0.09084593]\n",
      "Step 30 (386) @ Episode 7/10, action 6, reward -1,loss: 1.762586236\n",
      "[0.09084576 0.09084576 0.09084576 0.09084576 0.09084576 0.09084576\n",
      " 0.09084576 0.09154236 0.09084576 0.09084576 0.09084576]\n",
      "Step 31 (387) @ Episode 7/10, action 8, reward -1,loss: 1.74295330048\n",
      "[0.0908456 0.0908456 0.0908456 0.0908456 0.0908456 0.0908456 0.0908456\n",
      " 0.091544  0.0908456 0.0908456 0.0908456]\n",
      "Step 32 (388) @ Episode 7/10, action 4, reward -1,loss: 1.73800265789\n",
      "[0.09084544 0.09084544 0.09084544 0.09084544 0.09084544 0.09084544\n",
      " 0.09084544 0.09154564 0.09084544 0.09084544 0.09084544]\n",
      "Step 33 (389) @ Episode 7/10, action 6, reward -1,loss: 1.92519128323\n",
      "[0.09084527 0.09084527 0.09084527 0.09084527 0.09084527 0.09084527\n",
      " 0.09084527 0.09154727 0.09084527 0.09084527 0.09084527]\n",
      "Step 34 (390) @ Episode 7/10, action 8, reward -1,loss: 2.27910614014\n",
      "[0.09084511 0.09154891 0.09084511 0.09084511 0.09084511 0.09084511\n",
      " 0.09084511 0.09084511 0.09084511 0.09084511 0.09084511]\n",
      "Step 35 (391) @ Episode 7/10, action 5, reward -1,loss: 1.92881655693\n",
      "[0.09084495 0.09155055 0.09084495 0.09084495 0.09084495 0.09084495\n",
      " 0.09084495 0.09084495 0.09084495 0.09084495 0.09084495]\n",
      "Step 36 (392) @ Episode 7/10, action 6, reward -1,loss: 2.36326217651\n",
      "[0.09084478 0.09084478 0.09084478 0.09084478 0.09084478 0.09084478\n",
      " 0.09084478 0.09155218 0.09084478 0.09084478 0.09084478]\n",
      "Step 37 (393) @ Episode 7/10, action 3, reward 1,loss: 1.3067111969\n",
      "[0.09084462 0.09084462 0.09084462 0.09084462 0.09084462 0.09084462\n",
      " 0.09084462 0.09155382 0.09084462 0.09084462 0.09084462]\n",
      "Step 38 (394) @ Episode 7/10, action 8, reward -1,loss: 1.97518515587\n",
      "[0.09084445 0.09084445 0.09084445 0.09084445 0.09084445 0.09084445\n",
      " 0.09084445 0.09155546 0.09084445 0.09084445 0.09084445]\n",
      "Step 39 (395) @ Episode 7/10, action 4, reward -1,loss: 1.54715132713\n",
      "[0.09084429 0.09084429 0.09084429 0.09084429 0.09084429 0.09084429\n",
      " 0.09084429 0.09155709 0.09084429 0.09084429 0.09084429]\n",
      "Step 40 (396) @ Episode 7/10, action 0, reward 1,loss: 2.36855983734\n",
      "[0.09084413 0.09084413 0.09084413 0.09084413 0.09084413 0.09084413\n",
      " 0.09084413 0.09155873 0.09084413 0.09084413 0.09084413]\n",
      "Step 41 (397) @ Episode 7/10, action 9, reward -1,loss: 1.36807537079\n",
      "[0.09084396 0.09084396 0.09084396 0.09084396 0.09084396 0.09084396\n",
      " 0.09084396 0.09156036 0.09084396 0.09084396 0.09084396]\n",
      "Step 42 (398) @ Episode 7/10, action 2, reward -1,loss: 1.67554926872\n",
      "[0.0908438 0.0908438 0.0908438 0.0908438 0.0908438 0.0908438 0.0908438\n",
      " 0.091562  0.0908438 0.0908438 0.0908438]\n",
      "Step 43 (399) @ Episode 7/10, action 6, reward -1,loss: 1.2711366415\n",
      "[0.09084364 0.09084364 0.09084364 0.09084364 0.09084364 0.09084364\n",
      " 0.09084364 0.09156364 0.09084364 0.09084364 0.09084364]\n",
      "Step 44 (400) @ Episode 7/10, action 10, reward -3,loss: 1.98759400845\n",
      "Episode Reward: -21 Episode Length: 45\n",
      "[0.09084347 0.09084347 0.09084347 0.09084347 0.09084347 0.09084347\n",
      " 0.09084347 0.09156527 0.09084347 0.09084347 0.09084347]\n",
      "Step 0 (401) @ Episode 8/10, action 5, reward 1,loss: 1.99798643589\n",
      "[0.09084331 0.09156691 0.09084331 0.09084331 0.09084331 0.09084331\n",
      " 0.09084331 0.09084331 0.09084331 0.09084331 0.09084331]\n",
      "Step 1 (402) @ Episode 8/10, action 9, reward -1,loss: 1.74721121788\n",
      "[0.09084315 0.09156855 0.09084315 0.09084315 0.09084315 0.09084315\n",
      " 0.09084315 0.09084315 0.09084315 0.09084315 0.09084315]\n",
      "Step 2 (403) @ Episode 8/10, action 4, reward -1,loss: 1.39768970013\n",
      "[0.09084298 0.09157018 0.09084298 0.09084298 0.09084298 0.09084298\n",
      " 0.09084298 0.09084298 0.09084298 0.09084298 0.09084298]\n",
      "Step 3 (404) @ Episode 8/10, action 5, reward -1,loss: 1.08974897861\n",
      "[0.09084282 0.09157182 0.09084282 0.09084282 0.09084282 0.09084282\n",
      " 0.09084282 0.09084282 0.09084282 0.09084282 0.09084282]\n",
      "Step 4 (405) @ Episode 8/10, action 7, reward -1,loss: 1.94015669823\n",
      "[0.09084265 0.09157346 0.09084265 0.09084265 0.09084265 0.09084265\n",
      " 0.09084265 0.09084265 0.09084265 0.09084265 0.09084265]\n",
      "Step 5 (406) @ Episode 8/10, action 10, reward -3,loss: 1.06525969505\n",
      "Episode Reward: -6 Episode Length: 6\n",
      "[0.09084249 0.09157509 0.09084249 0.09084249 0.09084249 0.09084249\n",
      " 0.09084249 0.09084249 0.09084249 0.09084249 0.09084249]\n",
      "Step 0 (407) @ Episode 9/10, action 0, reward 1,loss: 1.57276344299\n",
      "[0.09084233 0.09157673 0.09084233 0.09084233 0.09084233 0.09084233\n",
      " 0.09084233 0.09084233 0.09084233 0.09084233 0.09084233]\n",
      "Step 1 (408) @ Episode 9/10, action 4, reward -1,loss: 2.5238571167\n",
      "[0.09084216 0.09157836 0.09084216 0.09084216 0.09084216 0.09084216\n",
      " 0.09084216 0.09084216 0.09084216 0.09084216 0.09084216]\n",
      "Step 2 (409) @ Episode 9/10, action 3, reward -1,loss: 2.83698129654\n",
      "[0.090842 0.09158  0.090842 0.090842 0.090842 0.090842 0.090842 0.090842\n",
      " 0.090842 0.090842 0.090842]\n",
      "Step 3 (410) @ Episode 9/10, action 0, reward -1,loss: 0.993740797043\n",
      "[0.09084184 0.09158164 0.09084184 0.09084184 0.09084184 0.09084184\n",
      " 0.09084184 0.09084184 0.09084184 0.09084184 0.09084184]\n",
      "Step 4 (411) @ Episode 9/10, action 7, reward -1,loss: 1.03001689911\n",
      "[0.09084167 0.09158327 0.09084167 0.09084167 0.09084167 0.09084167\n",
      " 0.09084167 0.09084167 0.09084167 0.09084167 0.09084167]\n",
      "Step 5 (412) @ Episode 9/10, action 0, reward -1,loss: 1.38550043106\n",
      "[0.09084151 0.09158491 0.09084151 0.09084151 0.09084151 0.09084151\n",
      " 0.09084151 0.09084151 0.09084151 0.09084151 0.09084151]\n",
      "Step 6 (413) @ Episode 9/10, action 10, reward -3,loss: 1.71963214874\n",
      "Episode Reward: -7 Episode Length: 7\n",
      "[0.09084135 0.09158655 0.09084135 0.09084135 0.09084135 0.09084135\n",
      " 0.09084135 0.09084135 0.09084135 0.09084135 0.09084135]\n",
      "Step 0 (414) @ Episode 10/10, action 0, reward 1,loss: 1.40303874016\n",
      "[0.09084118 0.09158818 0.09084118 0.09084118 0.09084118 0.09084118\n",
      " 0.09084118 0.09084118 0.09084118 0.09084118 0.09084118]\n",
      "Step 1 (415) @ Episode 10/10, action 7, reward -1,loss: 2.11656832695\n",
      "[0.09084102 0.09158982 0.09084102 0.09084102 0.09084102 0.09084102\n",
      " 0.09084102 0.09084102 0.09084102 0.09084102 0.09084102]\n",
      "Step 2 (416) @ Episode 10/10, action 1, reward 1,loss: 1.20527625084\n",
      "[0.09084085 0.09159146 0.09084085 0.09084085 0.09084085 0.09084085\n",
      " 0.09084085 0.09084085 0.09084085 0.09084085 0.09084085]\n",
      "Step 3 (417) @ Episode 10/10, action 1, reward -1,loss: 1.07207870483\n",
      "[0.09084069 0.09159309 0.09084069 0.09084069 0.09084069 0.09084069\n",
      " 0.09084069 0.09084069 0.09084069 0.09084069 0.09084069]\n",
      "Step 4 (418) @ Episode 10/10, action 7, reward -1,loss: 1.19845056534\n",
      "[0.09084053 0.09159473 0.09084053 0.09084053 0.09084053 0.09084053\n",
      " 0.09084053 0.09084053 0.09084053 0.09084053 0.09084053]\n",
      "Step 5 (419) @ Episode 10/10, action 4, reward -1,loss: 2.05976247787\n",
      "[0.09084036 0.09159637 0.09084036 0.09084036 0.09084036 0.09084036\n",
      " 0.09084036 0.09084036 0.09084036 0.09084036 0.09084036]\n",
      "Step 6 (420) @ Episode 10/10, action 2, reward -1,loss: 0.90350073576\n",
      "[0.0908402 0.091598  0.0908402 0.0908402 0.0908402 0.0908402 0.0908402\n",
      " 0.0908402 0.0908402 0.0908402 0.0908402]\n",
      "Step 7 (421) @ Episode 10/10, action 6, reward 1,loss: 1.31273901463\n",
      "[0.09084004 0.09159964 0.09084004 0.09084004 0.09084004 0.09084004\n",
      " 0.09084004 0.09084004 0.09084004 0.09084004 0.09084004]\n",
      "Step 8 (422) @ Episode 10/10, action 8, reward -1,loss: 1.34047698975\n",
      "[0.09083987 0.09160127 0.09083987 0.09083987 0.09083987 0.09083987\n",
      " 0.09083987 0.09083987 0.09083987 0.09083987 0.09083987]\n",
      "Step 9 (423) @ Episode 10/10, action 10, reward 3,loss: 1.70697259903\n",
      "Episode Reward: 0 Episode Length: 10\n",
      "New image is being loaded: 2009_001369.jpg\n",
      "[0.09083971 0.09083971 0.09083971 0.09083971 0.09083971 0.09083971\n",
      " 0.09083971 0.09160291 0.09083971 0.09083971 0.09083971]\n",
      "Step 0 (424) @ Episode 1/10, action 10, reward 3,loss: 1.21925473213\n",
      "Episode Reward: 3 Episode Length: 1\n",
      "[0.09083955 0.09083955 0.09083955 0.09083955 0.09083955 0.09083955\n",
      " 0.09083955 0.09160455 0.09083955 0.09083955 0.09083955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (425) @ Episode 2/10, action 4, reward 1,loss: 1.29619312286\n",
      "[0.09083938 0.09083938 0.09083938 0.09083938 0.09083938 0.09083938\n",
      " 0.09083938 0.09160618 0.09083938 0.09083938 0.09083938]\n",
      "Step 1 (426) @ Episode 2/10, action 0, reward 1,loss: 1.72959077358\n",
      "[0.09083922 0.09083922 0.09083922 0.09083922 0.09083922 0.09083922\n",
      " 0.09083922 0.09160782 0.09083922 0.09083922 0.09083922]\n",
      "Step 2 (427) @ Episode 2/10, action 7, reward -1,loss: 1.62579202652\n",
      "[0.09083905 0.09083905 0.09083905 0.09083905 0.09083905 0.09083905\n",
      " 0.09083905 0.09160946 0.09083905 0.09083905 0.09083905]\n",
      "Step 3 (428) @ Episode 2/10, action 6, reward -1,loss: 1.54344832897\n",
      "[0.09083889 0.09083889 0.09083889 0.09083889 0.09083889 0.09083889\n",
      " 0.09083889 0.09161109 0.09083889 0.09083889 0.09083889]\n",
      "Step 4 (429) @ Episode 2/10, action 7, reward 1,loss: 1.84018516541\n",
      "[0.09083873 0.09083873 0.09083873 0.09083873 0.09083873 0.09083873\n",
      " 0.09083873 0.09161273 0.09083873 0.09083873 0.09083873]\n",
      "Step 5 (430) @ Episode 2/10, action 4, reward -1,loss: 1.65895915031\n",
      "[0.09083856 0.09083856 0.09083856 0.09083856 0.09083856 0.09083856\n",
      " 0.09083856 0.09161437 0.09083856 0.09083856 0.09083856]\n",
      "Step 6 (431) @ Episode 2/10, action 8, reward -1,loss: 1.66883063316\n",
      "[0.0908384 0.0908384 0.0908384 0.0908384 0.0908384 0.0908384 0.0908384\n",
      " 0.091616  0.0908384 0.0908384 0.0908384]\n",
      "Step 7 (432) @ Episode 2/10, action 0, reward 1,loss: 1.10228729248\n",
      "[0.09083824 0.09083824 0.09083824 0.09083824 0.09083824 0.09083824\n",
      " 0.09083824 0.09083824 0.09083824 0.09083824 0.09161764]\n",
      "Step 8 (433) @ Episode 2/10, action 5, reward -1,loss: 1.35871458054\n",
      "[0.09083807 0.09161927 0.09083807 0.09083807 0.09083807 0.09083807\n",
      " 0.09083807 0.09083807 0.09083807 0.09083807 0.09083807]\n",
      "Step 9 (434) @ Episode 2/10, action 8, reward -1,loss: 1.68998777866\n",
      "[0.09083791 0.09162091 0.09083791 0.09083791 0.09083791 0.09083791\n",
      " 0.09083791 0.09083791 0.09083791 0.09083791 0.09083791]\n",
      "Step 10 (435) @ Episode 2/10, action 4, reward -1,loss: 1.44195389748\n",
      "[0.09083775 0.09162255 0.09083775 0.09083775 0.09083775 0.09083775\n",
      " 0.09083775 0.09083775 0.09083775 0.09083775 0.09083775]\n",
      "Step 11 (436) @ Episode 2/10, action 6, reward -1,loss: 1.04833757877\n",
      "[0.09083758 0.09162418 0.09083758 0.09083758 0.09083758 0.09083758\n",
      " 0.09083758 0.09083758 0.09083758 0.09083758 0.09083758]\n",
      "Step 12 (437) @ Episode 2/10, action 8, reward -1,loss: 1.3684296608\n",
      "[0.09083742 0.09083742 0.09083742 0.09083742 0.09083742 0.09083742\n",
      " 0.09083742 0.09162582 0.09083742 0.09083742 0.09083742]\n",
      "Step 13 (438) @ Episode 2/10, action 6, reward -1,loss: 2.52323174477\n",
      "[0.09083725 0.09162746 0.09083725 0.09083725 0.09083725 0.09083725\n",
      " 0.09083725 0.09083725 0.09083725 0.09083725 0.09083725]\n",
      "Step 14 (439) @ Episode 2/10, action 7, reward 1,loss: 1.7028683424\n",
      "[0.09083709 0.09083709 0.09083709 0.09083709 0.09083709 0.09083709\n",
      " 0.09083709 0.09162909 0.09083709 0.09083709 0.09083709]\n",
      "Step 15 (440) @ Episode 2/10, action 8, reward -1,loss: 1.23685848713\n",
      "[0.09083693 0.09163073 0.09083693 0.09083693 0.09083693 0.09083693\n",
      " 0.09083693 0.09083693 0.09083693 0.09083693 0.09083693]\n",
      "Step 16 (441) @ Episode 2/10, action 7, reward -1,loss: 1.53582668304\n",
      "[0.09083676 0.09163237 0.09083676 0.09083676 0.09083676 0.09083676\n",
      " 0.09083676 0.09083676 0.09083676 0.09083676 0.09083676]\n",
      "Step 17 (442) @ Episode 2/10, action 9, reward -1,loss: 1.53772473335\n",
      "[0.0908366 0.0908366 0.0908366 0.0908366 0.0908366 0.0908366 0.0908366\n",
      " 0.091634  0.0908366 0.0908366 0.0908366]\n",
      "Step 18 (443) @ Episode 2/10, action 5, reward -1,loss: 1.46360039711\n",
      "[0.09083644 0.09083644 0.09083644 0.09083644 0.09083644 0.09083644\n",
      " 0.09083644 0.09163564 0.09083644 0.09083644 0.09083644]\n",
      "Step 19 (444) @ Episode 2/10, action 4, reward -1,loss: 2.34385919571\n",
      "[0.09083627 0.09163727 0.09083627 0.09083627 0.09083627 0.09083627\n",
      " 0.09083627 0.09083627 0.09083627 0.09083627 0.09083627]\n",
      "Step 20 (445) @ Episode 2/10, action 2, reward 1,loss: 1.24702262878\n",
      "[0.09083611 0.09163891 0.09083611 0.09083611 0.09083611 0.09083611\n",
      " 0.09083611 0.09083611 0.09083611 0.09083611 0.09083611]\n",
      "Step 21 (446) @ Episode 2/10, action 2, reward 1,loss: 1.2024191618\n",
      "[0.09083595 0.09164055 0.09083595 0.09083595 0.09083595 0.09083595\n",
      " 0.09083595 0.09083595 0.09083595 0.09083595 0.09083595]\n",
      "Step 22 (447) @ Episode 2/10, action 1, reward 1,loss: 2.62412762642\n",
      "[0.09083578 0.09164218 0.09083578 0.09083578 0.09083578 0.09083578\n",
      " 0.09083578 0.09083578 0.09083578 0.09083578 0.09083578]\n",
      "Step 23 (448) @ Episode 2/10, action 5, reward -1,loss: 2.63170528412\n",
      "[0.09083562 0.09164382 0.09083562 0.09083562 0.09083562 0.09083562\n",
      " 0.09083562 0.09083562 0.09083562 0.09083562 0.09083562]\n",
      "Step 24 (449) @ Episode 2/10, action 10, reward -3,loss: 1.26736235619\n",
      "Episode Reward: -11 Episode Length: 25\n",
      "[0.09083545 0.09083545 0.09083545 0.09083545 0.09083545 0.09083545\n",
      " 0.09083545 0.09083545 0.09083545 0.09083545 0.09164546]\n",
      "Step 0 (450) @ Episode 3/10, action 10, reward 3,loss: 1.73014211655\n",
      "Episode Reward: 3 Episode Length: 1\n",
      "[0.09083529 0.09083529 0.09083529 0.09083529 0.09083529 0.09083529\n",
      " 0.09083529 0.09083529 0.09083529 0.09083529 0.09164709]\n",
      "Step 0 (451) @ Episode 4/10, action 8, reward 1,loss: 2.19470977783\n",
      "[0.09083513 0.09083513 0.09083513 0.09083513 0.09083513 0.09083513\n",
      " 0.09083513 0.09083513 0.09083513 0.09083513 0.09164873]\n",
      "Step 1 (452) @ Episode 4/10, action 8, reward -1,loss: 1.18606746197\n",
      "[0.09083496 0.09165037 0.09083496 0.09083496 0.09083496 0.09083496\n",
      " 0.09083496 0.09083496 0.09083496 0.09083496 0.09083496]\n",
      "Step 2 (453) @ Episode 4/10, action 1, reward -1,loss: 2.27822184563\n",
      "[0.0908348 0.091652  0.0908348 0.0908348 0.0908348 0.0908348 0.0908348\n",
      " 0.0908348 0.0908348 0.0908348 0.0908348]\n",
      "Step 3 (454) @ Episode 4/10, action 3, reward -1,loss: 0.900096297264\n",
      "[0.09083464 0.09165364 0.09083464 0.09083464 0.09083464 0.09083464\n",
      " 0.09083464 0.09083464 0.09083464 0.09083464 0.09083464]\n",
      "Step 4 (455) @ Episode 4/10, action 8, reward -1,loss: 1.37883806229\n",
      "[0.09083447 0.09165527 0.09083447 0.09083447 0.09083447 0.09083447\n",
      " 0.09083447 0.09083447 0.09083447 0.09083447 0.09083447]\n",
      "Step 5 (456) @ Episode 4/10, action 10, reward -3,loss: 1.92367053032\n",
      "Episode Reward: -6 Episode Length: 6\n",
      "[0.09083431 0.09083431 0.09083431 0.09083431 0.09083431 0.09083431\n",
      " 0.09083431 0.09165691 0.09083431 0.09083431 0.09083431]\n",
      "Step 0 (457) @ Episode 5/10, action 9, reward 1,loss: 1.58670544624\n",
      "[0.09083415 0.09165855 0.09083415 0.09083415 0.09083415 0.09083415\n",
      " 0.09083415 0.09083415 0.09083415 0.09083415 0.09083415]\n",
      "Step 1 (458) @ Episode 5/10, action 7, reward -1,loss: 1.03228449821\n",
      "[0.09083398 0.09166018 0.09083398 0.09083398 0.09083398 0.09083398\n",
      " 0.09083398 0.09083398 0.09083398 0.09083398 0.09083398]\n",
      "Step 2 (459) @ Episode 5/10, action 4, reward -1,loss: 1.92278110981\n",
      "[0.09083382 0.09166182 0.09083382 0.09083382 0.09083382 0.09083382\n",
      " 0.09083382 0.09083382 0.09083382 0.09083382 0.09083382]\n",
      "Step 3 (460) @ Episode 5/10, action 7, reward -1,loss: 1.73969316483\n",
      "[0.09083365 0.09166346 0.09083365 0.09083365 0.09083365 0.09083365\n",
      " 0.09083365 0.09083365 0.09083365 0.09083365 0.09083365]\n",
      "Step 4 (461) @ Episode 5/10, action 4, reward -1,loss: 1.65772366524\n",
      "[0.09083349 0.09166509 0.09083349 0.09083349 0.09083349 0.09083349\n",
      " 0.09083349 0.09083349 0.09083349 0.09083349 0.09083349]\n",
      "Step 5 (462) @ Episode 5/10, action 8, reward -1,loss: 1.74334096909\n",
      "[0.09083333 0.09166673 0.09083333 0.09083333 0.09083333 0.09083333\n",
      " 0.09083333 0.09083333 0.09083333 0.09083333 0.09083333]\n",
      "Step 6 (463) @ Episode 5/10, action 6, reward -1,loss: 1.93703842163\n",
      "[0.09083316 0.09083316 0.09083316 0.09166837 0.09083316 0.09083316\n",
      " 0.09083316 0.09083316 0.09083316 0.09083316 0.09083316]\n",
      "Step 7 (464) @ Episode 5/10, action 0, reward -1,loss: 1.57377552986\n",
      "[0.090833 0.090833 0.090833 0.09167  0.090833 0.090833 0.090833 0.090833\n",
      " 0.090833 0.090833 0.090833]\n",
      "Step 8 (465) @ Episode 5/10, action 8, reward -1,loss: 1.63254904747\n",
      "[0.09083284 0.09167164 0.09083284 0.09083284 0.09083284 0.09083284\n",
      " 0.09083284 0.09083284 0.09083284 0.09083284 0.09083284]\n",
      "Step 9 (466) @ Episode 5/10, action 1, reward -1,loss: 1.69022619724\n",
      "[0.09083267 0.09083267 0.09083267 0.09167327 0.09083267 0.09083267\n",
      " 0.09083267 0.09083267 0.09083267 0.09083267 0.09083267]\n",
      "Step 10 (467) @ Episode 5/10, action 1, reward -1,loss: 1.44623243809\n",
      "[0.09083251 0.09167491 0.09083251 0.09083251 0.09083251 0.09083251\n",
      " 0.09083251 0.09083251 0.09083251 0.09083251 0.09083251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 (468) @ Episode 5/10, action 6, reward -1,loss: 2.08698034286\n",
      "[0.09083235 0.09167655 0.09083235 0.09083235 0.09083235 0.09083235\n",
      " 0.09083235 0.09083235 0.09083235 0.09083235 0.09083235]\n",
      "Step 12 (469) @ Episode 5/10, action 8, reward -1,loss: 1.26870620251\n",
      "[0.09083218 0.09167818 0.09083218 0.09083218 0.09083218 0.09083218\n",
      " 0.09083218 0.09083218 0.09083218 0.09083218 0.09083218]\n",
      "Step 13 (470) @ Episode 5/10, action 1, reward -1,loss: 1.48154163361\n",
      "[0.09083202 0.09167982 0.09083202 0.09083202 0.09083202 0.09083202\n",
      " 0.09083202 0.09083202 0.09083202 0.09083202 0.09083202]\n",
      "Step 14 (471) @ Episode 5/10, action 6, reward -1,loss: 1.47921597958\n",
      "[0.09083185 0.09168146 0.09083185 0.09083185 0.09083185 0.09083185\n",
      " 0.09083185 0.09083185 0.09083185 0.09083185 0.09083185]\n",
      "Step 15 (472) @ Episode 5/10, action 10, reward -3,loss: 1.301684618\n",
      "Episode Reward: -16 Episode Length: 16\n",
      "[0.09083169 0.09083169 0.09083169 0.09083169 0.09083169 0.09083169\n",
      " 0.09083169 0.09083169 0.09083169 0.09083169 0.09168309]\n",
      "Step 0 (473) @ Episode 6/10, action 7, reward 1,loss: 1.40657699108\n",
      "[0.09083153 0.09083153 0.09083153 0.09083153 0.09083153 0.09083153\n",
      " 0.09083153 0.09083153 0.09083153 0.09083153 0.09168473]\n",
      "Step 1 (474) @ Episode 6/10, action 2, reward -1,loss: 2.46930909157\n",
      "[0.09083136 0.09083136 0.09083136 0.09083136 0.09083136 0.09083136\n",
      " 0.09083136 0.09083136 0.09083136 0.09083136 0.09168637]\n",
      "Step 2 (475) @ Episode 6/10, action 4, reward -1,loss: 1.17508172989\n",
      "[0.0908312 0.0908312 0.0908312 0.0908312 0.0908312 0.0908312 0.0908312\n",
      " 0.0908312 0.0908312 0.0908312 0.091688 ]\n",
      "Step 3 (476) @ Episode 6/10, action 10, reward 3,loss: 1.44272661209\n",
      "Episode Reward: 2 Episode Length: 4\n",
      "[0.09083104 0.09083104 0.09083104 0.09083104 0.09083104 0.09083104\n",
      " 0.09083104 0.09083104 0.09083104 0.09083104 0.09168964]\n",
      "Step 0 (477) @ Episode 7/10, action 3, reward 1,loss: 0.868938446045\n",
      "[0.09083087 0.09083087 0.09083087 0.09083087 0.09083087 0.09083087\n",
      " 0.09083087 0.09083087 0.09083087 0.09083087 0.09169127]\n",
      "Step 1 (478) @ Episode 7/10, action 2, reward -1,loss: 1.49922502041\n",
      "[0.09083071 0.09083071 0.09083071 0.09083071 0.09083071 0.09083071\n",
      " 0.09083071 0.09083071 0.09083071 0.09083071 0.09169291]\n",
      "Step 2 (479) @ Episode 7/10, action 0, reward 1,loss: 1.82091784477\n",
      "[0.09083055 0.09083055 0.09083055 0.09083055 0.09083055 0.09083055\n",
      " 0.09083055 0.09083055 0.09083055 0.09083055 0.09169455]\n",
      "Step 3 (480) @ Episode 7/10, action 4, reward -1,loss: 1.64322161674\n",
      "[0.09083038 0.09083038 0.09083038 0.09083038 0.09083038 0.09083038\n",
      " 0.09083038 0.09083038 0.09083038 0.09083038 0.09169618]\n",
      "Step 4 (481) @ Episode 7/10, action 0, reward -1,loss: 1.62193036079\n",
      "[0.09083022 0.09083022 0.09083022 0.09083022 0.09083022 0.09083022\n",
      " 0.09083022 0.09083022 0.09083022 0.09083022 0.09169782]\n",
      "Step 5 (482) @ Episode 7/10, action 2, reward -1,loss: 1.15162324905\n",
      "[0.09083005 0.09083005 0.09083005 0.09083005 0.09083005 0.09083005\n",
      " 0.09083005 0.09083005 0.09083005 0.09083005 0.09169946]\n",
      "Step 6 (483) @ Episode 7/10, action 6, reward -1,loss: 1.64121103287\n",
      "[0.09082989 0.09082989 0.09082989 0.09082989 0.09082989 0.09082989\n",
      " 0.09082989 0.09082989 0.09082989 0.09082989 0.09170109]\n",
      "Step 7 (484) @ Episode 7/10, action 0, reward -1,loss: 1.59309744835\n",
      "[0.09082973 0.09082973 0.09082973 0.09082973 0.09082973 0.09082973\n",
      " 0.09082973 0.09082973 0.09082973 0.09082973 0.09170273]\n",
      "Step 8 (485) @ Episode 7/10, action 2, reward 1,loss: 1.29703021049\n",
      "[0.09082956 0.09082956 0.09082956 0.09082956 0.09082956 0.09082956\n",
      " 0.09082956 0.09082956 0.09082956 0.09082956 0.09170437]\n",
      "Step 9 (486) @ Episode 7/10, action 4, reward 1,loss: 1.44008743763\n",
      "[0.0908294 0.0908294 0.0908294 0.0908294 0.0908294 0.0908294 0.0908294\n",
      " 0.0908294 0.0908294 0.0908294 0.091706 ]\n",
      "Step 10 (487) @ Episode 7/10, action 8, reward -1,loss: 1.94538402557\n",
      "[0.09082924 0.09082924 0.09082924 0.09082924 0.09082924 0.09082924\n",
      " 0.09082924 0.09082924 0.09082924 0.09082924 0.09170764]\n",
      "Step 11 (488) @ Episode 7/10, action 1, reward 1,loss: 1.42159724236\n",
      "[0.09082907 0.09082907 0.09082907 0.09082907 0.09082907 0.09082907\n",
      " 0.09082907 0.09082907 0.09082907 0.09082907 0.09170927]\n",
      "Step 12 (489) @ Episode 7/10, action 2, reward -1,loss: 2.13269519806\n",
      "[0.09082891 0.09082891 0.09082891 0.09082891 0.09082891 0.09082891\n",
      " 0.09082891 0.09082891 0.09082891 0.09082891 0.09171091]\n",
      "Step 13 (490) @ Episode 7/10, action 0, reward 1,loss: 1.42917585373\n",
      "[0.09082875 0.09082875 0.09082875 0.09082875 0.09082875 0.09082875\n",
      " 0.09082875 0.09082875 0.09082875 0.09082875 0.09171255]\n",
      "Step 14 (491) @ Episode 7/10, action 9, reward -1,loss: 1.45353066921\n",
      "[0.09082858 0.09171418 0.09082858 0.09082858 0.09082858 0.09082858\n",
      " 0.09082858 0.09082858 0.09082858 0.09082858 0.09082858]\n",
      "Step 15 (492) @ Episode 7/10, action 0, reward -1,loss: 1.55815136433\n",
      "[0.09082842 0.09171582 0.09082842 0.09082842 0.09082842 0.09082842\n",
      " 0.09082842 0.09082842 0.09082842 0.09082842 0.09082842]\n",
      "Step 16 (493) @ Episode 7/10, action 5, reward -1,loss: 1.51512646675\n",
      "[0.09082825 0.09171746 0.09082825 0.09082825 0.09082825 0.09082825\n",
      " 0.09082825 0.09082825 0.09082825 0.09082825 0.09082825]\n",
      "Step 17 (494) @ Episode 7/10, action 5, reward -1,loss: 1.66476666927\n",
      "[0.09082809 0.09171909 0.09082809 0.09082809 0.09082809 0.09082809\n",
      " 0.09082809 0.09082809 0.09082809 0.09082809 0.09082809]\n",
      "Step 18 (495) @ Episode 7/10, action 6, reward -1,loss: 1.50077807903\n",
      "[0.09082793 0.09172073 0.09082793 0.09082793 0.09082793 0.09082793\n",
      " 0.09082793 0.09082793 0.09082793 0.09082793 0.09082793]\n",
      "Step 19 (496) @ Episode 7/10, action 1, reward -1,loss: 1.2727638483\n",
      "[0.09082776 0.09172237 0.09082776 0.09082776 0.09082776 0.09082776\n",
      " 0.09082776 0.09082776 0.09082776 0.09082776 0.09082776]\n",
      "Step 20 (497) @ Episode 7/10, action 6, reward -1,loss: 0.851599097252\n",
      "[0.0908276 0.091724  0.0908276 0.0908276 0.0908276 0.0908276 0.0908276\n",
      " 0.0908276 0.0908276 0.0908276 0.0908276]\n",
      "Step 21 (498) @ Episode 7/10, action 8, reward -1,loss: 1.57222294807\n",
      "[0.09082744 0.09082744 0.09082744 0.09082744 0.09082744 0.09082744\n",
      " 0.09082744 0.09172564 0.09082744 0.09082744 0.09082744]\n",
      "Step 22 (499) @ Episode 7/10, action 1, reward -1,loss: 1.14813709259\n",
      "[0.09082727 0.09082727 0.09082727 0.09082727 0.09082727 0.09082727\n",
      " 0.09082727 0.09172727 0.09082727 0.09082727 0.09082727]\n",
      "Step 23 (500) @ Episode 7/10, action 0, reward -1,loss: 1.17315506935\n",
      "[0.09082711 0.09082711 0.09082711 0.09082711 0.09082711 0.09082711\n",
      " 0.09082711 0.09082711 0.09082711 0.09082711 0.09172891]\n",
      "Step 24 (501) @ Episode 7/10, action 3, reward 1,loss: 0.825629353523\n",
      "[0.09082695 0.09082695 0.09082695 0.09082695 0.09082695 0.09082695\n",
      " 0.09082695 0.09082695 0.09082695 0.09082695 0.09173055]\n",
      "Step 25 (502) @ Episode 7/10, action 4, reward -1,loss: 1.87803316116\n",
      "[0.09082678 0.09082678 0.09082678 0.09082678 0.09082678 0.09082678\n",
      " 0.09082678 0.09082678 0.09082678 0.09082678 0.09173218]\n",
      "Step 26 (503) @ Episode 7/10, action 0, reward -1,loss: 1.56966078281\n",
      "[0.09082662 0.09082662 0.09082662 0.09173382 0.09082662 0.09082662\n",
      " 0.09082662 0.09082662 0.09082662 0.09082662 0.09082662]\n",
      "Step 27 (504) @ Episode 7/10, action 0, reward -1,loss: 1.25958251953\n",
      "[0.09082645 0.09082645 0.09082645 0.09082645 0.09082645 0.09082645\n",
      " 0.09082645 0.09082645 0.09082645 0.09082645 0.09173546]\n",
      "Step 28 (505) @ Episode 7/10, action 1, reward -1,loss: 1.54292953014\n",
      "[0.09082629 0.09082629 0.09082629 0.09173709 0.09082629 0.09082629\n",
      " 0.09082629 0.09082629 0.09082629 0.09082629 0.09082629]\n",
      "Step 29 (506) @ Episode 7/10, action 4, reward -1,loss: 1.20351958275\n",
      "[0.09082613 0.09082613 0.09082613 0.09173873 0.09082613 0.09082613\n",
      " 0.09082613 0.09082613 0.09082613 0.09082613 0.09082613]\n",
      "Step 30 (507) @ Episode 7/10, action 1, reward -1,loss: 1.46834206581\n",
      "[0.09082596 0.09082596 0.09082596 0.09174037 0.09082596 0.09082596\n",
      " 0.09082596 0.09082596 0.09082596 0.09082596 0.09082596]\n",
      "Step 31 (508) @ Episode 7/10, action 0, reward -1,loss: 0.81002086401\n",
      "[0.0908258 0.0908258 0.0908258 0.091742  0.0908258 0.0908258 0.0908258\n",
      " 0.0908258 0.0908258 0.0908258 0.0908258]\n",
      "Step 32 (509) @ Episode 7/10, action 2, reward 1,loss: 1.88375174999\n",
      "[0.09082564 0.09082564 0.09082564 0.09174364 0.09082564 0.09082564\n",
      " 0.09082564 0.09082564 0.09082564 0.09082564 0.09082564]\n",
      "Step 33 (510) @ Episode 7/10, action 1, reward -1,loss: 0.99659794569\n",
      "[0.09082547 0.09174527 0.09082547 0.09082547 0.09082547 0.09082547\n",
      " 0.09082547 0.09082547 0.09082547 0.09082547 0.09082547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34 (511) @ Episode 7/10, action 3, reward 1,loss: 1.54351890087\n",
      "[0.09082531 0.09174691 0.09082531 0.09082531 0.09082531 0.09082531\n",
      " 0.09082531 0.09082531 0.09082531 0.09082531 0.09082531]\n",
      "Step 35 (512) @ Episode 7/10, action 9, reward -1,loss: 1.13557291031\n",
      "[0.09082515 0.09174855 0.09082515 0.09082515 0.09082515 0.09082515\n",
      " 0.09082515 0.09082515 0.09082515 0.09082515 0.09082515]\n",
      "Step 36 (513) @ Episode 7/10, action 6, reward -1,loss: 1.6006102562\n",
      "[0.09082498 0.09082498 0.09082498 0.09175018 0.09082498 0.09082498\n",
      " 0.09082498 0.09082498 0.09082498 0.09082498 0.09082498]\n",
      "Step 37 (514) @ Episode 7/10, action 9, reward -1,loss: 2.1209602356\n",
      "[0.09082482 0.09082482 0.09082482 0.09175182 0.09082482 0.09082482\n",
      " 0.09082482 0.09082482 0.09082482 0.09082482 0.09082482]\n",
      "Step 38 (515) @ Episode 7/10, action 9, reward -1,loss: 1.42466533184\n",
      "[0.09082465 0.09082465 0.09082465 0.09175346 0.09082465 0.09082465\n",
      " 0.09082465 0.09082465 0.09082465 0.09082465 0.09082465]\n",
      "Step 39 (516) @ Episode 7/10, action 5, reward -1,loss: 1.40727138519\n",
      "[0.09082449 0.09082449 0.09082449 0.09175509 0.09082449 0.09082449\n",
      " 0.09082449 0.09082449 0.09082449 0.09082449 0.09082449]\n",
      "Step 40 (517) @ Episode 7/10, action 8, reward -1,loss: 0.828827083111\n",
      "[0.09082433 0.09082433 0.09082433 0.09175673 0.09082433 0.09082433\n",
      " 0.09082433 0.09082433 0.09082433 0.09082433 0.09082433]\n",
      "Step 41 (518) @ Episode 7/10, action 10, reward -3,loss: 2.26839876175\n",
      "Episode Reward: -26 Episode Length: 42\n",
      "[0.09082416 0.09082416 0.09082416 0.09175837 0.09082416 0.09082416\n",
      " 0.09082416 0.09082416 0.09082416 0.09082416 0.09082416]\n",
      "Step 0 (519) @ Episode 8/10, action 6, reward 1,loss: 1.23752367496\n",
      "[0.090824 0.090824 0.090824 0.090824 0.090824 0.090824 0.090824 0.090824\n",
      " 0.090824 0.090824 0.09176 ]\n",
      "Step 1 (520) @ Episode 8/10, action 7, reward 1,loss: 1.15472221375\n",
      "[0.09082384 0.09082384 0.09082384 0.09082384 0.09082384 0.09082384\n",
      " 0.09082384 0.09082384 0.09082384 0.09082384 0.09176164]\n",
      "Step 2 (521) @ Episode 8/10, action 7, reward -1,loss: 1.63875985146\n",
      "[0.09082367 0.09082367 0.09082367 0.09082367 0.09082367 0.09082367\n",
      " 0.09082367 0.09082367 0.09082367 0.09082367 0.09176327]\n",
      "Step 3 (522) @ Episode 8/10, action 1, reward -1,loss: 0.823807954788\n",
      "[0.09082351 0.09082351 0.09082351 0.09082351 0.09082351 0.09082351\n",
      " 0.09082351 0.09082351 0.09082351 0.09082351 0.09176491]\n",
      "Step 4 (523) @ Episode 8/10, action 9, reward -1,loss: 1.13755989075\n",
      "[0.09082335 0.09082335 0.09082335 0.09082335 0.09082335 0.09082335\n",
      " 0.09082335 0.09082335 0.09082335 0.09082335 0.09176655]\n",
      "Step 5 (524) @ Episode 8/10, action 3, reward 1,loss: 1.47833442688\n",
      "[0.09082318 0.09082318 0.09082318 0.09082318 0.09082318 0.09082318\n",
      " 0.09082318 0.09082318 0.09082318 0.09082318 0.09176818]\n",
      "Step 6 (525) @ Episode 8/10, action 5, reward -1,loss: 1.28363227844\n",
      "[0.09082302 0.09082302 0.09082302 0.09176982 0.09082302 0.09082302\n",
      " 0.09082302 0.09082302 0.09082302 0.09082302 0.09082302]\n",
      "Step 7 (526) @ Episode 8/10, action 4, reward -1,loss: 1.2910220623\n",
      "[0.09082285 0.09082285 0.09082285 0.09177146 0.09082285 0.09082285\n",
      " 0.09082285 0.09082285 0.09082285 0.09082285 0.09082285]\n",
      "Step 8 (527) @ Episode 8/10, action 8, reward -1,loss: 1.51703357697\n",
      "[0.09082269 0.09082269 0.09082269 0.09177309 0.09082269 0.09082269\n",
      " 0.09082269 0.09082269 0.09082269 0.09082269 0.09082269]\n",
      "Step 9 (528) @ Episode 8/10, action 6, reward -1,loss: 1.80096817017\n",
      "[0.09082253 0.09082253 0.09082253 0.09177473 0.09082253 0.09082253\n",
      " 0.09082253 0.09082253 0.09082253 0.09082253 0.09082253]\n",
      "Step 10 (529) @ Episode 8/10, action 7, reward 1,loss: 1.1503944397\n",
      "[0.09082236 0.09082236 0.09082236 0.09177637 0.09082236 0.09082236\n",
      " 0.09082236 0.09082236 0.09082236 0.09082236 0.09082236]\n",
      "Step 11 (530) @ Episode 8/10, action 7, reward 1,loss: 0.855051934719\n",
      "[0.0908222 0.0908222 0.0908222 0.091778  0.0908222 0.0908222 0.0908222\n",
      " 0.0908222 0.0908222 0.0908222 0.0908222]\n",
      "Step 12 (531) @ Episode 8/10, action 0, reward 1,loss: 1.15463232994\n",
      "[0.09082204 0.09082204 0.09082204 0.09177964 0.09082204 0.09082204\n",
      " 0.09082204 0.09082204 0.09082204 0.09082204 0.09082204]\n",
      "Step 13 (532) @ Episode 8/10, action 5, reward -1,loss: 1.81838095188\n",
      "[0.09082187 0.09082187 0.09082187 0.09178127 0.09082187 0.09082187\n",
      " 0.09082187 0.09082187 0.09082187 0.09082187 0.09082187]\n",
      "Step 14 (533) @ Episode 8/10, action 3, reward 1,loss: 1.72501683235\n",
      "[0.09082171 0.09082171 0.09082171 0.09178291 0.09082171 0.09082171\n",
      " 0.09082171 0.09082171 0.09082171 0.09082171 0.09082171]\n",
      "Step 15 (534) @ Episode 8/10, action 10, reward -3,loss: 1.11039686203\n",
      "Episode Reward: -4 Episode Length: 16\n",
      "[0.09082155 0.09082155 0.09082155 0.09082155 0.09082155 0.09082155\n",
      " 0.09082155 0.09082155 0.09082155 0.09082155 0.09178455]\n",
      "Step 0 (535) @ Episode 9/10, action 5, reward 1,loss: 1.24947166443\n",
      "[0.09082138 0.09082138 0.09082138 0.09082138 0.09082138 0.09082138\n",
      " 0.09082138 0.09082138 0.09082138 0.09082138 0.09178618]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-497615e0a092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# Perform gradient descent update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mstates_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step {} ({}) @ Episode {}/{}, action {}, reward {},loss: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step {} ({}) @ Episode {}/{}, action {}, reward {},loss: {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/active localization/oneObj_xImg/lib/DNN.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, sess, s, a, y)\u001b[0m\n\u001b[0;32m    179\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[0;32m    180\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             feed_dict)\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# Old API for using on cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes=10  #200     \n",
    "replay_memory_size=500000    #2500   \n",
    "replay_memory_init_size=500 #500  \n",
    "update_target_estimator_every=10000 #100  \n",
    "discount_factor=0.99\n",
    "epsilon_start=1.0\n",
    "epsilon_end=0.1\n",
    "epsilon_decay_steps=500000 #10000  \n",
    "batch_size=32\n",
    "category = \"cat\"\n",
    "\n",
    "#model_name = \"defaul_DQL_architecture_epis{}_memorySize{}_UTE{}_EDS{}\".format(num_episodes, replay_memory_size, update_target_estimator_every, epsilon_decay_steps)\n",
    "model_name = \"cats\"\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(model_name))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "\n",
    "\n",
    "done = False\n",
    "elist = []\n",
    "rlist = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    # Old API: sess.run(tf.initialize_all_variables())  \n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    num_located = 0\n",
    "\n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    #stats = plotting.EpisodeStats(\n",
    "        #episode_lengths=np.zeros(num_episodes),\n",
    "        #episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    report_path = os.path.join(experiment_dir, \"report\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(report_path):\n",
    "        os.makedirs(report_path)\n",
    "    f = open(report_path+\"/log.txt\", 'w')\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    #print \"init:{}\".format(total_t)\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "    episode_counter = 0\n",
    "    #mybreak = 0\n",
    "    \n",
    "    for indx,tmp in enumerate(extractData(category, \"train\", batch_size)):\n",
    "\n",
    "        #if mybreak > 5:\n",
    "            #break\n",
    "        \n",
    "        img=tmp[0]\n",
    "        target=tmp[1]\n",
    "\n",
    "        im2 = Image.frombytes(\"RGB\",(img['image_width'],img['image_height']),img['image'])\n",
    "        env = ObjLocaliser(np.array(im2),target)\n",
    "        print \"New image is being loaded: {}\".format(img['image_filename'])\n",
    "        \n",
    "        if len(replay_memory) < replay_memory_init_size:\n",
    "            \n",
    "            # Populate the replay memory with initial experience\n",
    "            print(\"Populating replay memory...\\n\")\n",
    "\n",
    "            env.Reset(np.array(im2))\n",
    "            state = env.wrapping()\n",
    "\n",
    "\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "\n",
    "            for i in range(replay_memory_init_size):\n",
    "\n",
    "                #env.Reset(np.array(im2))\n",
    "                #state = env.wrapping()\n",
    "                #state = state_processor.process(sess, state)\n",
    "                #state = np.stack([state] * 4, axis=2)\n",
    "                #action = 0\n",
    "                #counter = 0\n",
    "                #done = False\n",
    "\n",
    "                #while (action != 10) or (counter < 50):\n",
    "\n",
    "                action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "\n",
    "                #next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "                reward = env.takingActions(VALID_ACTIONS[action])\n",
    "                next_state = env.wrapping()\n",
    "\n",
    "                if action == 10:\n",
    "                    done = True\n",
    "                else: \n",
    "                    done = False\n",
    "\n",
    "                next_state = state_processor.process(sess, next_state)\n",
    "                next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "                replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "                #counter += 1\n",
    "\n",
    "                if done:\n",
    "                    #state = env.reset()\n",
    "                    env.Reset(np.array(im2))\n",
    "                    state = env.wrapping()\n",
    "                    state = state_processor.process(sess, state)\n",
    "                    state = np.stack([state] * 4, axis=2)\n",
    "                else:\n",
    "                    state = next_state \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            # Save the current checkpoint\n",
    "            saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "            # Reset the environment\n",
    "            #state = env.reset()\n",
    "            env.Reset(np.array(im2))\n",
    "            state = env.wrapping()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "            loss = None\n",
    "            t=0\n",
    "            action = 0\n",
    "            e = 0\n",
    "            r = 0\n",
    "            #done = False\n",
    "            # One step in the environment\n",
    "            while (action != 10) and (t < 50):\n",
    "                #print \"hello22:{}\".format(loss)\n",
    "                #env.drawActions()\n",
    "                # Epsilon for this time step\n",
    "                epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "                # Maybe update the target estimator\n",
    "                if total_t % update_target_estimator_every == 0:\n",
    "                    estimator_copy.make(sess)\n",
    "                    print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "                # Print out which step we're on, useful for debugging.\n",
    "                #sys.stdout.flush()\n",
    "\n",
    "                # Take a step\n",
    "                #print epsilon\n",
    "                action_probs = policy(sess, state, epsilon)\n",
    "                print action_probs\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "\n",
    "\n",
    "                #next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "                reward = env.takingActions(VALID_ACTIONS[action])\n",
    "                next_state = env.wrapping()\n",
    "                if action == 10:\n",
    "                    done = True\n",
    "                else: \n",
    "                    done = False\n",
    "\n",
    "\n",
    "                next_state = state_processor.process(sess, next_state)\n",
    "                next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "                # If our replay memory is full, pop the first element\n",
    "                if len(replay_memory) == replay_memory_size:\n",
    "                    replay_memory.pop(0)\n",
    "\n",
    "                # Save transition to replay memory\n",
    "                replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "                # Update statistics\n",
    "                #stats.episode_rewards[i_episode] += reward\n",
    "                #stats.episode_lengths[i_episode] = t\n",
    "\n",
    "                # Sample a minibatch from the replay memory\n",
    "                samples = random.sample(replay_memory, batch_size)\n",
    "                states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "                # Calculate q values and targets\n",
    "                q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "                targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "                # Perform gradient descent update\n",
    "                states_batch = np.array(states_batch)\n",
    "                loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "                print(\"Step {} ({}) @ Episode {}/{}, action {}, reward {},loss: {}\".format(t, total_t, i_episode + 1, num_episodes, action, reward, loss))\n",
    "                f.write(\"Step {} ({}) @ Episode {}/{}, action {}, reward {},loss: {}\\n\".format(t, total_t, i_episode + 1, num_episodes, action, reward, loss))     \n",
    "\n",
    "\n",
    "                # Counting number of correct localsied objects\n",
    "                if reward == 3:\n",
    "                    num_located += 1\n",
    "\n",
    "                state = next_state\n",
    "                t += 1\n",
    "                total_t += 1\n",
    "                e = e + loss\n",
    "                r = r + reward\n",
    "        \n",
    "            episode_counter += 1\n",
    "\n",
    "            # Add summaries to tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "            episode_summary.value.add(simple_value=r, tag=\"episode/reward\")\n",
    "            episode_summary.value.add(simple_value=t, tag=\"episode/length\")\n",
    "            episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "            episode_summary.value.add(simple_value=current_process.memory_percent(), tag=\"system/v_memeory_usage_percent\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, episode_counter)\n",
    "            q_estimator.summary_writer.flush()\n",
    "\n",
    "            #prin_stats = plotting.EpisodeStats(\n",
    "                #episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "                #episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "            #print(\"Episode Reward: {} Episode Length: {}\".format(prin_stats.episode_rewards[-1], prin_stats.episode_lengths[-1]))\n",
    "            #f.write(\"Episode Reward: {} Episode Length: {}\".format(prin_stats.episode_rewards[-1], prin_stats.episode_lengths[-1]))\n",
    "            print(\"Episode Reward: {} Episode Length: {}\".format(r, t))\n",
    "            f.write(\"Episode Reward: {} Episode Length: {}\".format(r, t))\n",
    "\n",
    "            elist.append(float(e)/t)\n",
    "            rlist.append(float(r)/t)\n",
    "        #mybreak += 1\n",
    "\n",
    "        \n",
    "f.close()\n",
    "print \"number of correct located objects:{}\".format(num_located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"episods\")\n",
    "plt.ylabel(\"avg reward per epi\")\n",
    "plt.title(\"num of correct obj localisation:{0}\".format(num_located))\n",
    "plt.plot(rlist)\n",
    "plt.savefig(\"./graphs/reward\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"episods\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.plot(elist)\n",
    "plt.savefig(\"./graphs/error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to code: https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/Deep%20Q%20Learning%20Solution.ipynb\n",
    "\n",
    "https://www.oreilly.com/ideas/reinforcement-learning-with-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
